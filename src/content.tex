\section{Introduction}
\begin{quotation}
\textit{Imagine an earthquake has hit a populous area and many major roads are
impassable. Buildings are severely damaged or collapsed, and fire has broken out
in many places. At the crisis management center, response professionals are
working together to coordinate the earthquake relief effort. You are the
incident commander charged with coordinating the search
and rescue teams working in the field. There is a large tabletop display in
front of you, showing the map of the site. Information coming from the field is
updated on the display in real-time.}

\textit{A report about a big explosion at a chemical plant comes in and you move
the map around, zoom in and rotate it to get a good view of the plant. On the
map, you see there is a group of unmanned vehicles nearby. After selecting them
on the map with your hand, you speak to the interface ``Go nearer to the
explosion site to gather more information,'' while tracing the route the
vehicles should take to avoid obstacles. Then you instruct rescue team No. 3 to evacuate the residents
in the sorrounding buildings by going under a bridge because the surface of the
bridge is blocked. You gesture with one hand as the bridge and the other
hand moving under it to emphasize this.}
\end{quotation}

The scenario above is an example in the Urban Search and Rescue (USAR) domain.
It shows an application of a multi-modal interface to a real-world problem. Gestures play an important part in this scenario,
providing key information about location, method and timing of movements,
and about spatial relationship among the objects being described.

Recent trends in user interfaces have brought on a new wave of interaction
techniques that depart from the traditional mouse and keyboard that have been 
used for decades. These include multi-touch interfaces such as the 
iPhone, the iPad and the Microsoft 
Surface\textsuperscript{\textregistered} as well as camera-based systems such as
the Microsoft Kinect and the Nintendo\textsuperscript{\textregistered} Wii. Most
of these devices gained instant popularity among consumers, and the common trait
among them is that they make interacting with computation more natural and 
effortless. All these devices allow users to use their hands and/or body 
gestures to directly manipulate the virtual objects. It feels more natural this 
way because this is how we interact with our environment in everyday life.
 
Our goal is to take this aspiration to the next level by developing an
intelligent multimodal interface for natural interaction. By \textit{natural
interaction}, we mean the kind of cognitively transparent, effortless multimodal
communication that can happen between people; we want to make this possible in
human-computer interaction such that the computer interface understands what the
user is saying and doing, and the user can simply behave. We believe that
natural interaction can provide better learnability, flexibility, memorability,
convenience and efficiency, but further user studies are needed to investigate
this belief.

Gesture plays an important part in multimodal interaction, especially for
conveying spatial information. The focus of this thesis is developing a
hierarchical approach for continous gesture analysis that can be easily
applied in different domains and applications. Specifically, we focus on
gestures made with hands. 

I propose to develop a gesture-based interface that allows users to perform
manipulative and communicative gestures continuously with no arbitrary
restrictions. The key elements of the approach are the hierarchical framework
for gestural analysis based on abrastract hidden Markov models (AHMMs) and the
accurate real-time detection of the onset of natural gestures that convey 
information intended by the users while filtering out movements that do not 
convey information.

\section{Background}
\subsection{Definition of Gestures}
Webster's Dictionary defines gestures as ``\ldots the use of motions of the
limbs or body as a mean of expression; a movement usually of the body or limbs
that expresses or emphasizes an idea, sentiment, or attitude.'' This definition
is particularly related to the communicational aspect of the human hand and body
movments. However, in the domain of human computer interaction (HCI) the notion
of gestures is somewhat different. In thier review of visual interpretation of hand
gestures for HCI, Pavlovic et al. \cite{Pavlovic97} states that in a computer
controlled environment one wants to use the human hand to perform tasks that
mimic both the natural use of the hand as a manipulator, and its use in
human-machine communication. They include both manipulative and communicative
gestures in their gesture taxonomy. This is also the definition we will use in
this thesis.

Pavlovic et al. \cite{Pavlovic97} also gives a model (Figure. 
\ref{fig:gesture_production}) for the production and perception of gestures 
based on the model used in the field of spoken language recognition. According 
to model, gestures originates as a gesturer's mental concept, possibly in 
conjunction with speech. They are expressed through the motion of arms and 
hands. Also, observers perceive gesetures as streams of visual images which they
interpret using the knowledge they possess about those gestures. In HCI, the 
observer is the computer and the knowledge it possesses is the training data we 
give it.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/gesture_production.png} 
  \caption{Production and perception of gestures. Hand gestures originate as a
  mental concept, are expressed through arm and hand motion, and are perceived
  as visual images \cite{Pavlovic97}.}
  \label{fig:gesture_production}
\end{figure}

\subsection{Gestural Taxonomy}\label{sec:taxonomy}
The mental concept of a gesture represents the semantic meaning or the
actual intention of the gesturer. In this dimension, hand movements can be
divided into different general categories. Each category has its own characteristics and requires different responses from the computer interface. Having a systematic gesture taxonomy can inform
us about the design of the interactive system. The taxonomy will also be the
basis of our hierarchical approach to gesture interpretation. Our gesture
taxonomy (see Figure \ref{fig:taxonomy}) is based on the one developed by Pavlovic et al.
\cite{Pavlovic97} with some changes to make the terminology clearer. This is
also similar to the \textit{nature} dimension in the taxonomy proposed by
Wobbrock et al.
\cite{wobbrock09}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/taxonomy.png} 
  \caption{Gesture taxonomy}
  \label{fig:taxonomy}
\end{figure}

First, gestures, as \textit{intentional} movements, should be distinguished from
\textit{unintentional} hand movements (like beats). By \textit{unintentional}
movements, we mean movements that are not intended to convey information. In
contrast, gestures are meaningful hand movements that people make to convey some
information. The distinction is important for a natural interface because there
should not be any restriction on how people should place or move their hands
when they are not doing any meaningful gestures. Then the gestures are further 
divided into manipulative and communicative categories.

Manipulative gestures are the ones used to act on objects, while communicative 
ones have an inherent communicational purpose \cite{Pavlovic97}. For
manipulative gestures, the system needs to respond frame by frame while the user is doing
certain direct manipulations. This can be achieved by tracking the hand
state in real-time. For communicative gestures, the system needs to repond to
the meaning of the gesture when the user finishes performing the gesture. This part is
accomplished by gesture recognition. So manipulative gestures and communicative
gestures fall into the \textit{continuous} category and the
\textit{discrete} category respectively in the \textit{flow} dimension of the
taxonomy proposed by Wobbrock \cite{wobbrock09} et al. based on their
user study.

People perform communicative gestures via acts or symbols. Gesutre via acts are
those directly related to the interpretation of the movement itself. Such
movements are classified as either mimetic (which imitate some actions) or
deictic (pointing acts that convey spatial information). Gestures via symbols
are those that have a linguistic role, and are often represented by different static hand postures. An example is forming the
O.K. pose for accept. 

The physical arm and hand movements that express the mental concept of the
gesture can be categorized into different \textit{forms} \cite{wobbrock09} as
shown in Table \ref{tab:form}. Both manipulative and communicative gestures can
have all of these forms. This shows that the features we want to use for
gestural analysis should be based on the hand pose and the locus of the hand
movement.

\begin{table}[h]
  \centering
  \begin{tabular}{| l | l |}
  	\hline
  	\textbf{Form} 		  & \textbf{Description} \\ \hline 
  	Static pose  		  & Hand pose is held in one location. \\ \hline
  	Dynamic pose 		  & Hand pose changes in one location. \\ \hline
  	Static pose and path  & Hand pose is held as hand moves. \\ \hline
  	Dynamic pose and path & Hand pose changes as hand moves. \\ \hline
  \end{tabular}
  \caption{Different gesture forms.}
  \label{tab:form}
\end{table}

\section{Research Questions}
This thesis focuses on several research questions that aim to solve some of the
key challenges in developing a multimodal interface with natual gesture input.

\begin{itemize}
  \item \textbf{Fine-grained hand pose estimation.} The hand model for
  gestural input can range from a very simplisitic one (one point) to a very
  comprehensive one (a 26 degrees of freedom (DOF) sekeleton). There is a trade
  off between the accuracy of the model and comoputational complexity. The question
  here is what hand model is sufficient for the HCI tasks for both manipulative
  and communicative gestures.
  \item \textbf{Online gesture spotting.} A real-time gesture-driven HCI
  application requires gesture spotting with minimum delay. Hence, it is
  desirable that the gesture spotting is done online using only the current and
  the past movement data. The questions are how to accurately determine the
  start and the end of a gesture and how to filter out unintentional hand
  movement.
  \item \textbf{Unified hierarchical model for gestural analysis.} We propose a
  unified framework based on AHMMs that distinguishes unintentional movement,
  manipulative gestures and communicative gesture in real-time and provides
  appropriate responses to the user.
\end{itemize}

\section{Related Work}
There are many active research on using hand as an input modality for
human-cormputer interaction. To do this, the computer needs to be able to
interpret the dynamic and/or static configurations of the human hand, arm, and
even other parts of the human body. We break down the gestural input
system into several sequential modules in a pipeline as in Figure
\ref{fig:pipeline}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/pipeline.png} 
  \caption{Gestural input pipeline.}
  \label{fig:pipeline}
\end{figure}

\subsection{Sensors}
The first step in the pipeline is having sensors that capture the hand movements
and configurations, converting analog signals to digital signals. First attemps
to solve this problem resulted in mechanical devices that directly measure hand
joint angles and spatial positions. This group is best represented by the
glove-based approaches using devices such us CyberGloves \cite{fels09} and
Powergloves \cite{kadous02}. However, wearing gloves makes gesturing more
combersome, and many efforts have been made to make the gloves more
light-weight, for example, by using bluetooth wireless data transmission (e.g.
CyberGove II). To further reduce the bulkyness of the gloves, people use colored markers on the fingers
\cite{mistry09} or colored gloves with no electronics \cite{Wang09}, and use RGB
cameras and computer vision techniques to interpret gestures. However, by
requiring the user wearing something extra still hinders the acceptance of
such devices as ``everyday'' natural interaction interfaces. 

The most nonobtrusive way to capture the hand is bare-hand tracking. Shin et al. \cite{Shin04} use
stereo RGB cameras to extract the hand from background based on the skin color. One
limitation of RGB cameras is that they are very sensitive to lighting
conditions. This prompted researchers to look into other types of cameras. Oka et al. 
\cite{Oka02} use thermal imaging for hand segmentation under complex
background and changing light, relying on the condition that a hand's
temperature is almost always distinct from the background. Their approach does
not detect finger contact with the surface. Larson et al. \cite{larson11}
improve on this method to detect the finger contact by using
the heat transferred from a user's hand to the surface for touch-based gestures.
However, inorder to detect the head trace, the user has to drag the fingers a
bit instead of just touching. This may be a small departure from what users
would expect as ``natural'' based on their experience in the physical world.

Thermal imaging measures radiation emitted by objects in the
\textit{far}-infrared (F-IR) spectrum. There are other well-known ``IR-imaging''
techniques used in the HCI community which use devices that operate in the \textit{near}-infrared (N-IR) spectrum.
N-IR is employed in some fairly recent interactive tabletop surfaces and depth
cameras. A number of projects in the HCI community have used IR for tabletop
interaction by detecting multi-touch gestures using an under mounted
camera and an illumination source. An exmaple of this is Microsoft's 
Surface\textsuperscript{\textregistered}. Recently, portable multi-touch devices
such as phones and tablets have become more and more ubiquitous. These devices
are based on capacitve touch sensitive screens. Touch-based devices are becoming
more and more mature, however the kind of gestures one can use are still
limited. The gestures are usually limited in 2D space with one or multiple
fingers. 

Going beyond the limitation of touch-only gestures, researchers at Microsoft
augmented the Surface technology with a switchable diffuser, additional
strips of IR LEDS with a different wave length for diffuse illumination, and an
additional IR sensitive camera which images IR reflected from the diffuse
illumination of the environment \cite{hilliges09}. In this way, they can capture
the hand above the surface as well. The height of the hand is estimated based on
the pixel intensity.

Since the introduction of Kinect, a motion sensing input device by Microsoft for
the Xbox 360 video game console, researchers in the HCI community, as well as
many independent hackers, have been using Kinect's depth sensor for capturing
both body and hand gestures \cite{openni}. Since then, there are also many
similar devices coming into the market which have been used for capturing
gestures. For instance, Harrison et al. \cite{harrison11} use  a short-range
PrimeSense \cite{primesense} depth camera for their wearable multitouch
interaction. In comparison to the aforementioned augmented Surface setup,
Kinect, and the likes, provide a cheap alternative for depth sensing. In this
thesis, we will explore the potential of using the Kinect sensor for detecting 
both the touch-based gesture and above-surface 3D gestures. Potentially, we can 
use the depth information for determing surface touch, and hence, eliminate the 
need for the complicated electronics of a touch sensitive screen. This also 
enables the gestural interaction on a large tabletop display, much bigger than 
that of Microsoft's Surface.

\subsection{Hand Tracking}
After getting input from the sensor(s), the next step in the pipeline is
tracking the hand(s). This is essentially the frame-by-frame estimation of the
parameters of the hand model based on the sensor input. The complexity of the
hand model is application dependent. For a given application, a very coarse and
simple model may be sufficient. The simplest model is treating the hand as a
blob and only the 2D/3D location of the blob is tracked. For example, Sharma et
al. \cite{sharma00} use 2D positional and time differential parameters to track
the hands as blobs which is sufficient for them to distinguish whether the
hands are doing a point, a contour or a circle gesture. The PrimeSense NITE
middleware for OpenNI's natural interaction framework also tracks the hand as a
point. It requires the user to do a ``focus'' gesture (``click'' or ``wave'')
to gain control in order to start hand tracking \cite{primesense-manual}. The
gestures it supports are ``click'', ``wave'', ``swipe left'', ``swipe right'', 
``raise hand candidate'' and ``hand candidate moved''.

However, to make a more generalizable system for natural-like interaction, a
more sophisticated model is required. One step forward is adding fingertip
locations in the hand model as exmplified in \cite{Oka02} \cite{harrison11}
\cite{larson11}. Tracking fingertips is usually sufficient for manipulating
objects on the 2D surface. However, for a more rich set of gestures, the one
that also involves communicative gestures, we may need a more sophisticated
model. For instance, Wang et al. \cite{Wang09} uses a 26 DOFs
3D skeletal model in their real-time hand-tracking system. 

Another approach is using appearance-based model. This means that the model
parameters are not directly derived from the 3D spatial description of the hand.
The hand poses are modeled by relating the appearance of any pose to the 
appearance of the set of predefined, template poses \cite{Pavlovic97}. In
their markless hand-tracking system, Wang et al. \cite{wang11} use efficient
queries of a database of gestures and desktop-specific hand silhouette samples
for pinch/click gesture detection.

In our system, we propose a combination of both approaches. A simplified 3-D
skeletal hand model is useful for manipulative gestures because we want to know 
exactly where the fingertips are and the grabbing and releasing poses of the hand. For
communicative gestures, we just need to know the meaning of the gesture instead
of the exact spatial parameters. Hence example-based template models would be
more suitable which also require less computation.

\subsection{Gesture Spotting}
To distinguish unintentional movements from intentional
movements, a common approach is to use one or two nongesture HMMs to
provide likelihood thresholds for outlier rejection. For instance, in \cite{yang07}, a weak
universal movement model has been used for adaptive thresholding.
Peng et al. \cite{peng11} argue that using one or two HMMs cannot
effectively reject complex outliers, e.g., when they resemble portions of a
gesture. In addition to a general garbage gesture model, They train several nongesture HMM models by
automatically identifying and manually specifying nongesture models from the
traning data. This approach is suitable for idenfiying dynamic communicative
gestures and when the set of the input data are limit. They only test on the
given dataset, but in real life the possible unintentional movements are
limitless and it is not clear how this approach will scale. In addition,
using nongesture HMMs only works for discrete communicative gestuers, but not
for static or continuous manipulative gestures.

Using feature selection methods (e.g. principle component analysis), we will
investigate the fundamental distinctions between unintentional and intentional
hand movements based on hand and arm poses and movements and the context of the
interaction. Then we will train a model for unintentional movement based on the
distinguishing features.

\subsection{Gesture Recognition}
Most of the gestural input applications have been focusing on tracking only
\cite{harrison11} \cite{larson11}. For hand gesture recognition, much of the
work is on synthetic gestures, most notably sign language recognition
\cite{Bauer00} \cite{kadous02}. Wang et al. \cite{Wang09} made a simple
demonstration of sign language finger spelling with their color glove
hand-tracking system. For dynamic gesture recognition, hidden markov model (HMM) is a commonly used
technique because it is suitable for time series data \cite{sharma00}. 

Wang et al. \cite{wang06} argued that a significant limitation of the
HMM is the requirement of conditional independence of observations. In
addition, HMM, as a generative model, optimizes the likehood of
generating all the examples of a given gesture class, which is not
necessarily optimal for discriminating the gesture class against other
gestures. They introduced the use of hidden conditional random fields (HCRF),
an extension of conditional random fields (CRF), for gesture recognition, and
showed improved accuracy for the set of head and arm gestures they use in comparison to using HMMs. 
However, this approach does not handle continuous input. 

Morency et al. \cite{morency07} presented a latent-dynamic conditional
random field (LDCRF) model that is able to perform sequence labeling and
segmentation simultaneously. However, the forward-backward message-passing
shedule used in belief propagation limited to its use to bounded input
sequences only. Song et al. \cite{song12} extended LDCRF with multi-layered
filtering to make it work to the unbounded input domain. However, their approach
still incurs some delay, e.g., one to four seconds in their experiments. For
real-time interaction, 0.1 second is about the limit for having the user feel
that the system is reacting instantaneously. We may relax this response time a
bit, but 1.0 second is about the limit for the user's flow of thought to stay
uninterruped \cite{card91}.  Also they only focus on communicative
gestures and assume no unintentional gestures in the input data.

Another limitation of CRF-based models is that training for these models is much
more computatoinally expensive and converges much slower than those of HMMs 
\cite{lafferty01}. To combine the benefits of both the HMM-based model and the 
discriminative model, we propose to use discriminative traning for the HMM-based
model instead.

There is not much prior work that actually distinguishes manipulative gestures
and communcative gestures. Oka et al. \cite{Oka02} developed a system that allows
both direct manipulation and symbolic gestures. Based on the tracking result,
the system first differentiates the gesture as either manipulative or symbolic 
according to the extension of the thumb. They regard gestures with an extended 
thumb as direct manipulation and those with a bent thumb as symbolic gestures. 
For direct manipulation, the system selects operating modes such as rotate, 
move or resize based on the fingertips configuration; for symbolic gestures, it 
uses HMM for classification. The way they distinguish manipulative and 
communicative gestures seems to be arbitrary and ``unnatural''. They did that 
probably for the ease of it because they are only tracking fingertips.
Instead, we propose an interface that can respond to manipulative and
communicative gestures seamlessly based on the inherent difference between the two.

\subsection{Multimodal Systems}
Bolt's pioneering work in the ``Put That There'' system \cite{Bolt80} 
demonstrated the potential for voice and gestural interaction.  In that system, 
the hand position and orientation were tracked by the Polhemus tracker, i.e.,
the hand was essentially transformed to a point on the screening. The actual hand 
posture did not matter, even if it was not in a pointing shape. The speech also 
followed a rigid and limited command-like grammar. Even though this is early 
work, it provides some insight about the advantages of multimodal interaction. 
As Bolt summarized in the paper, using pointing gesture allows the use of 
pronouns in the speech, with the corresponding gain in naturalness and economy 
of expression \cite{Bolt80}.

Since then, several multimodal interaction prototypes were 
developed that moved beyond Bolt's ``Put That There'' system. Cohen et al. 
\cite{Cohen97} developed the QuickSet prototype which is a collaborative, 
multimodal system running on a hand-held PC using pen and voice as input. They 
used a novel multimodal integration strategy that allows speech and pen gesture 
to compensate for each other, yielding a more robust system. Rauschert et al. 
\cite{Rauschert02} developed a system called Dialogue-Assisted Visual 
Environment for Geoinformation (DAVE\_G) that uses free hand gestures and speech
as input. They recognized that gestures are more useful for expressing spatial 
relations and locations. Gestures in DAVE\_G include pointing, indicating an 
area and outlining contours. Speech and gesture are fused for commands that need
spatial information provided by the gesture. 

In this thesis, we will also explore the fusion of speech and gestures. In
addition to use the deicitc gestures to provide spatial information as a complement to
speech as in \cite{Rauschert02}, we will also explore the use of speech as a
complement to manipulative gestures based on the finding from the user study
done by Yin et al.\cite{yin10}. They observe that manipulative gestures are at times
accompanied by adjectives and adverbs that refine the actions.

\section{Proposed Work and Procedure}
Our long term goal is to build an intelligent multimodal interface for natural
interaction that can serve as a testbed for enabling the formulation of a more
principled system design framework for multimodal HCI. One focus of this thesis
is on the gestural input modality for a large tabletop display. The user should
be able to use gesture as an input effortlessly for both direct manipulation and communication to the system.

\subsection{System Setup}
The custom tabletop structure includes four $1280\times1024$ pixel projectors 
(Dell 5100MP) that provide a $2560\times2048$ pixel resolution display. The
display is projected onto a flat white surface digitizer (GTCO Calcomp DrawingBoard V), 
which uses a stylus as an input device. The digitizer is tilted 10 degrees down 
in front, and is placed at 41in (104cm) above the floor, following FAA's design 
standard to accommodate the $5^{th}$ through $95^{th}$ percentiles of 
population. The projected displays were mechanically aligned to produce a single 
seamless large display area. The graphics card used is AMD
Radeon\texttrademark{TM} HD 6870 and the operating system used is Ubuntu 11.10.

\begin{figure}
  \centering
  \subfigure[] {
	\includegraphics[width=0.4\textwidth]{figures/setup1.png} 
  }
  \subfigure[] {
  	\includegraphics[width=0.4\textwidth]{figures/setup_close.png}
  }
  \caption{System setup.} \label{fig:setup}
\end{figure}

One Kinect motion sensor by Microsoft is placed above the center of the tabletop
at the same level of the projectors. Figure~\ref{fig:setup} shows the setup. The
Kinect sensor has a RGB camera, a depth sensor and a multi-array microphone.
We use the depth sensor to capture the hand motion data because it is
less sensitive to the lighting condition. This is particularly useful for our
projection system. The Dell 5100MP projector uses a spinning color wheel to
modulate the image. This produces a visible artifact on the screen, referred to 
as the ``rainbow effect'', with colors separating out in distinct red, green, 
and blue. At any given instant in time, the image on the screen is either red, or green, or blue,
and the technology relies upon people's eyes not being able to detect the rapid 
changes from one to the other. However, when seen through a RGB camera, the
effect is very obvious, and this can greatly affect hand segmentation if we were to use 
the RGB images. 

The Kinect sensor outputs video at a frame rate of 30Hz. The depth sensing video
stream has a resolution of $640\times 480$ pixels with 11-bit depth value. The
depth value increases as the distance of the object from the sensor increases.
The depth sensor has a range limit of about 0.5m - 5m with a resolution of 1.5mm
at 0.5m and 5cm at 5m. The tabletop surface is about 1.2m away from the Kinect
sensor which allows us to have a relatively good depth resolution. We use the
open source OpenNI framework \footnote{https://github.com/OpenNI/OpenNI} and its
Kinect driver \footnote{https://github.com/avin2/SensorKinect} to get both the depth and RGB data streams.

\subsection{Kinect Calibration}
In order to develop the interactive interface, we need to map the point in the
depth image to the point on the display. We do this by projecting a
checkerboard image on the tabletop display, and placing some wooden blocks at
the corners of the checkerboard image to create the depth differences so that 
the depth sensor can capture these corners (see Figure~\ref{fig:calibration}).
We manually labeled 16 pairs of corresponding points on the display and the depth image. Then we
apply undistortion to the depth image and planar homography to find the mapping.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/calibration.png} 
  \caption{Kinect calibration. The darker squares are the wooden blocks. The
  intensity of the gray level image is inversely proportional to the distance
  from the Kinect sensor.}
  \label{fig:calibration}
\end{figure}

Planar homography is a projective mapping from one plane to another. In our
case, we are mapping points on the plane of the depth image to the points
on the plane of the display. To evaluate the result of the calibration, we
obtain a new set of manualy labeled corresponding points on the display and the
depth image. We then transform the coordinates of the points on the depth image
to the coordinates on the display using the calibration result, and find the
Euclean distance (error) between the transformed coordinates and the labeled
coordindates of the points on the display. The average errors in the x-axis and
y-axis are 4.3px and 8.9px respectively, which are 0.21cm, and 0.4cm in physical distance of the
projected display. The average width of the index fingertip is about 1.4cm. So
the error is less than 30\% of the width of the fingertip. 

\subsection{Hand Tracking}
The hand tracking process consists of feature detection and parameter estimation. The hand tracking
pipeline consists the following steps:

\begin{enumerate}
  \item Background subtraction
  \item Forelimb and hand segmentation
  \item Fingertips tracking
\end{enumerate}

The following subsections explain in details about these steps. Many of the
computer vision methods we use are based on the OpenCV
\footnote{http://opencv.willowgarage.com/wiki/} library and the Java interface 
JavaCV \footnote{http://code.google.com/p/javacv/}.

\subsubsection{Background Subtraction}
The background which is the tabletop is relatively static, but there is still 
noise from the depth sensor. We use the averaging background method which
learns the average and average difference of each pixel in the depth image as
the model of the background. The average values are based on the initial 30
frames with no hands in the scene. For the subsequent frames, any value that is 
6 times the average frame-to-frame absolute difference below the average tabletop depth value for that pixel is considered 
foreground because it is closer to the sensor above.

To clean up the background subtracted depth image, we use 1 iteration of
morphological opening to clear out small pixel noise. Morphological opening is a
combination of erosion and dilation. Both erosion and dilation are morphological
transformations. The kernel of erosion is a \textit{local minimum} operator,
while that of dilation is a \textit{local maximum} operator.

\begin{figure}[h]
  \centering
  \subfigure[Without using morphological opening.] {
	\includegraphics[width=0.4\textwidth]{figures/background_subtraction.png} 
  }
  \subfigure[Using morpholocial opening to clear out small pixel noise.] {
  	\includegraphics[width=0.4\textwidth]{figures/morphology.png}
  }
  \caption{Background subtraction.} \label{fig:setup}
\end{figure}


\subsubsection{Forelimb and Hand Segmentation}
With the cleaned up background subtracted depth image, we find connected
components by finding all contours that are not too small. These components are
considered to be the forelimbs and are approximated with convex hulls and 
bounding boxes. The hand region is at either end of the bounding box depending
on the position of the arm relative to the table.

\subsubsection{Fingertips Tracking}
We base our estimation of the hand model on the geometric property of the
hand. We compute the convexity defects from the convex hull of the forelimb.
From this we can observe that for an extended finger, it has one convexity
defect on each side and the two adjacent sides of the defects form an
acute angle. We iterate through the adjacent convexity defects, and mark the
intersection of those sides that forms an angle that is smaller than a threshold
value as the potential fingertip. We further refine the fingertip position by
searching in the general direction of the finger and finding the sharp change in
the gradient of the depth value.


\subsubsection{Kalman Filter}
We use Kalman filter to further improve the tracking accuracy. We use a
$4$-dimentional vector, including x, y coordinates and the velocity in x, y
coordinates of the fingertip, as the dynamic state of the fingertip.
The measurement is a $2$-dimensional vector of the measured x, y
coordinates of the fingertip. 

Assuming no external control, the a priori estimate $x_k^-$ of the state is
given by:
\begin{align*}
x_k^- = Fx_{k - 1} + w_k
\end{align*}
where $F$ is the $4$-by-$4$ \textit{transfer matrix} characterizing the
dynamics of the system with the following values

\begin{align*}
F = \left[ \begin{array}{cccc}
	1 & 0 & 1 & 0 \\
	0 & 1 & 0 & 1 \\
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 1 \end{array} \right]
\end{align*}
and $w_k$ is the \textit{process noise} associated with random events or forces
that directly affect the actual state of the system. We assume that the 
components of $w_k$ have Gaussian distribution $N(0, Q_k)$ for some $4$-by-$4$ 
covariance matrix $Q_k$.

Using $P_k^-$ to denote the error covariance, then a priori estimate for this
covariance at time $k$ is obtained from the value at time $k - 1$ by:
\begin{align*}
P_k^- = FP_{k - 1}F^T + Q_{k - 1}
\end{align*}

We set the matrix Q with the following values:
\begin{align*}
Q = \left[ \begin{array}{cccc}
	1 & 0 & 0 & 0 \\
	0 & 1 & 0 & 0 \\
	0 & 0 & 10 & 0 \\
	0 & 0 & 0 & 10 \end{array} \right]
\end{align*}
  
The result of parameter estimation of the hand model is used both as the input
to the gesture recognizer and the system response to manipulative gestures. One
research question we want to investigate is what level of complexity of the
hand model is needed for the manipulative task. Do we need a 26 DOFs skeleton
model, or a simplified model with a smaller number of DOFs would suffice. Is
tracking fingertips, or even just tracking the contact area of the hand for
surface gestures enough for the manipulative tasks?

\subsection{Hierachical Approach to Real-Time Gestural Analysis}
As mentioned in Section \ref{sec:taxonomy}, we propose a systematic approach to
enabling gestural interaction according to the hierarchical taxonomy of
gestures. Unintentional movements should be ignored by the system.
Manipulative gestures require frame-by-frame tracking of the hand so that the
system can make real-time update to the virtual object being manipulated according to the hand
movement. Communicative gestures require system response after the user acts.

The research question here is how we can first differentiate the broader
categories of gestures, and then further classify the gestures into more 
specific categories. One possibility is to explore the inherent difference
between unintentional movements, manipulative gestures and communicative
gestures. For examples, the speed of the manipulative gestures tend to be
slower; unintentional movements tend to be fast and repetitive. We can also
utilize the context contraints to narrow down the possibility of the gestures.
For example, if the hand is at the position of a movable object, the probablity
for it to be a manipulative gesture is higher. We can combine both the inherent
hand movement features and the context constraints to build a probabilistic
model to differentiate the broader categories of the hand movements.

The rationale for breaking down the hand movements according to the broader
categories is that the features that are relevant to each categories are
different. For manipulative gestures, we need to track the positions and 
orientation of the fingertips and the hand so that we can update the position 
and orientation of the virtual object being manipulated. For diectic gestures, 
we need to know the general pointing direction. For symbolic gestures, both the 
static hand poses and the dynamic hand movement need to be considered as 
features in the models. We propose that using different models for
different categories can improve the performance of the system. 

We combine different levels of recoginition using the abstract hidden Markov
model (AHMM) architecture. The AHMM is a probabilistic model used to explain the
interaction between behaviors at different levels of abstraction \cite{johns05}.
We can map the levels in our gesture taxonomy into the levels of abstraction in
an AHMM. The higher abstract levels represent the mental intention of the
gesturer. We need to determine what is the optimal number of levels to use in
AHMM. We can use the number that resemble the hierarchy of our taxonomy. But the
greater the number, the greater the computational complexity. A flatter model
may be more reasonable and does not affect accuracy very much. The bottom level in the
AHMM consists of observations and states in a typical HMM. HMM is
suitable for modeling sequential data such as time series. It has been used widely for dynmic gesture recognition with reasonable 
success. Kalman filter model is widely used for tracking objects. It can viewed 
as a special case of HMM where state transition probabilities and emission 
probabilities are all Gaussian. As a result, we propose to combine these two 
together under a unified framework of AHMM where Kalman filter is used for 
tracking hands for manipulative gestures and HMM is used for dynamic gesture 
recognition. We also treat static gesture as a special case of dynamic gesture 
with only one state.

We will start with a simple 1-level AHMMs \cite{murphy02} first (Figure
\ref{fig:amms}). $G_t$ represents the mental concept of the gesture that the
gesturer currently has. It includes unintentional movement ($U$), manipulative gesture ($M$), and various
communicative gestures ($C_i$). $S_t$ is the hidden state of the hand pose and
movement, which is essentially a vector quantization of the actual, observed 
(but noisy) feature vector $X_t$. $F_t^G$ is a binary indicator variable that is
``on'' (has value 1) if the lower level HMM at time $t$ has just ``finished''
(i.e., is about to enter an end state), otherwise it is ``off'' (has value 0).

\tikzstyle{vertex}=[circle, draw, minimum size=16pt, inner sep=0pt]
\tikzstyle{observed-vertex}=[circle, draw, minimum size=16pt, inner sep=0pt,
							 fill=black!20] 
\tikzstyle{edge} = [draw, thick, -]
\tikzstyle{directed-edge} = [draw, thick, ->]

\begin{figure}[h]
\centering
  \begin{tikzpicture}[auto,swap, scale=2]
    % First we draw the vertices
    \foreach \pos/\name in {{(0, 2)/G_1}, {(1, 2)/G_2}, {(2,2)/G_3},
    	{(0.5,1.5)/F_1^G}, {(1.5,1.5)/F_2^G}, {(2.5, 1.5)/F_3^G},
    	{(0, 1)/S_1}, {(1, 1)/S_2}, {(2, 1)/S_3}} 
      \node[vertex] (\name) at \pos {$\name$};
    \foreach \pos/\name in {{(0, 0)/X_1}, {(1, 0)/X_2}, {(2, 0)/X_3}}
      \node[observed-vertex] (\name) at \pos {$\name$};
    % Connect vertices with edges and draw weights
    \foreach \source/ \dest in {G_1/S_1, S_1/X_1, G_1/F_1^G, S_1/F_1^G,
    							F_1^G/G_2, G_1/G_2, G_2/S_2, S_2/X_2, G_2/F_2^G,
    							S_2/F_2^G, F_2^G/G_3, G_2/G_3, G_3/S_3, S_3/X_3,
    							G_3/F_3^G, S_3/F_3^G, S_1/S_2, S_2/S_3} 
      \path[directed-edge] (\source) -- (\dest);
  \end{tikzpicture}
  \caption{A 1-level AHMM. $F_t^G$ turns on if state $S_t$ satisfies goal
  $G_t$; this causes a new goal to be chosen. However, the new state may depend
  on the old state; hence there is no arc from $F_{t-1}^G$ to $S_t$ to
  ``reset'' the submodel \cite{murphy02}.}
  \label{fig:amms}
\end{figure}

\subsection{Discriminative Training}
HMMs as a generative model allows us to model the joint
distribution of the observed sequence. Traditionally,
maximum-likelihood (ML) estimation is used to learn the parameters of HMMs. However, our real task is to
classify the sequence of the observed data. Discriminative classifiers model the
posterior $p(y|x)$ directly, and hence is believed to give lower error rate. In
the speech recognition community, discriminative training methods have been
proposed and have shown significant improvements over ML-trained models on many
large vocabulary speech recognition tasks \cite {chang12}. To further improve
the discriminative power of our model, we use discriminative training methods
for gestural analysis.

HMM provides a computationally efficient modeling framework. The HMM makes two
main assumptions:

1. There exists a hidden state sequence, $S$, that forms a Markov chain that
generates the observation vectors.

2. Given the state $s_t$ at any time point, the corresponding observation $x_t$
is conditionally independent of other observations and states.

Instead of seeking model parameters that can maximize the likelihood of the
data, discriminative training methods seek parameters that can minimize the
confusions that occur in the training data. In general, discriminative training
methods consist of two steps: First, construct a smooth and efficient computable
objective function that reflects the degree of confusion; second, adjust the
model parameters such that the objective function can be optimized. 

We will explore several commonly used discriminative training criteria such as
minimum classification error training, maximum mutual
information training, and margin-based training methods. For optimizing
parameters, we can use either extended Baum-Welch method or gradient-based
methods. We will also compare the results to those without discriminative
training and those with CRF-based models.

\subsection{Gesture Spotting}
\textit{Gesture spotting} is a method that
distinguishes meaningful gestures from unintentional movements \cite{kang04}.
Hand movements are signified by motion. We can detect the start and the end of a hand movement by the amount of motion. This forms a segment
which is a candidate hand movement. 

Instead of using hard coded threshold values for detecting candidate
start and end frame as in \cite{kang04}, we use machine learning method to make
the result more robust. We propose to use support vector machine (SVM) to
classify each frame into either a candidate start, candidate end or neither. One
important question here is what features we should use for the feature vector.
We will experiement with the criteria used in \cite{kang04} which are motion
energy, static poses, and curvature, and will also explore other possible
features such as positions relative to the body or table, and accompanying
speech etc.

In order to combine the result of SVM with our probabilistic model, we use the
probabilistic output from SVM as proposed by Platt \cite{platt99}.

\subsection{Multimodal Interaction with Gesture and Speech} 
For this thesis, we focus on gestural input and interaction. However, we will
also add some basic speech recognition capability to combine speech and gestures
for some USAR interaction scenarios. We will use an off-the-shelf speech 
recognizer, and investigate how to combine both speech and gesture to make
the interaction more natural and effortless. Specifically, we will explore the
alignment of speech and gesture, mutual disambiguation of speech and gesture,
and how speech and gestures compliement each other in certain scenarios.

\subsection{Motivating Application}
We will develop a simple game-like application that simulates some of the USAR
interaction scenarios. The game will be browser-based for easy access. With the
recent development in HTML5, browser-based interaction becomes richer and
richer. Web browsers have become the main application people use to do various
tasks because it simply provides better accessibility to information via the
Internet. We believe that being able to use gesture as an input to the browser
will make a big contribution to HCI as well.

The game we have in mind is highly inspired by
StarCraft\footnote{http://us.battle.net/sc2/en/}. StarCraft is a real-time
strategic game that involves controling different kinds of units to accomplish a vast array of tasks. The game's richness in command
and control can help us develop a set of gestural language in this domain.

The gestural input functionality we want demonstrate in this application are:

\begin{enumerate}
  \item Moving units from one location to another
  \item Panning the map
  \item Command a unit to build different kinds of buildings, like recycler,
  barrack, and factory etc.
  \item Command units to attack the enemies by doing an attack gesture (say
  ``attack'') and pointing to the location of attack
\end{enumerate}

\subsection{User Study}\label{sec:user_study}
We will bootstrap the system with a set of natural gestures defined based on the
user studies done by Yin et al. \cite{yin10} and Wobbrock et al.
\cite{wobbrock09} on the set of natural gestures people do for surface
computing. We will conduct a user study after making a simple appplication where
the user can perform both manipulative and communicative gestures. The goal of 
the user study is to evaluate the accuracy of the hand tracking and gesture 
recognition. A research question here is whether we can bootstrap the system 
only with the geseture examples we provide ourselves, or we need to collect 
training examples from the users, and how well the system performs 
independent of users.

\section{Schedule}
\begin{enumerate}
  \item Improve fingertips tracking \hfill Done by April 01, 2012

  Improve the Kalman filter method by adjusting the noise covariance matrix
  based on the confidence of the feature extraction. Finish multiple fingertips
  tracking and quantitaive evaluation of the accuracy of the tracking for both
  touch and non-touch interations.

  \item Detection of start and end of gestures \hfill Done by May 28, 2012

  \item Model unintentional gestures \hfill Done by Oct 01, 2012

  \item Develop AHMMs framework	\hfill Done by Jan 01, 2013

  Develop the AHMMs hierachical model and combine the model for start and
  end of hand movements, the model for unintentional movement and HMM models for
  communicative gestures. 
  
  \item Descriminative training and evaluation \hfill Done by Mar 01, 2013
  
  \item Develop browser-based game application \hfill Done by May 01, 2013
    
  \item Combine speech with gesture	\hfill Done by Jun 01, 2013

  \item User study \hfill Done by Jul 01, 2013

  Conduct user study described in Section \ref{sec:user_study}.
  
  \item Write the first draft of thesis report \hfill Done by Sep 01, 2013
  \item Complete thesis report	\hfill Done by Oct 01, 2013
  \end{enumerate}

\section{Principal Equipment and Facilities}
Here is a list of equipment and facilities needed for the study:

\begin{enumerate}
  \item A white surface digitizer (GTCO Calcomp DrawingBoard V)
  \item Four $1280\times1024$ pixel projectors (Dell 5100MP)
  \item One Kinect motion sensor
  \item One desktop with a powerful graphics card (1GB memory, at least 4 ports)
\end{enumerate}

All the above equipment are avalaible.

\section{Appendix A - Review of Kalman Filter}
Let $x_k$ be an $n$-dimentional vector of state components and $P_k$ be the
$n$-by-$n$ error covariance. The measurements $z_k$ is a $m$-dimensional
vector given by:
\begin{align*}
z_k = H_kx_k + v_k,
\end{align*}
where $H_k$ is an $m$-by-$n$ matrix and $v_k$ is the measurement error.

The \textit{Kalman gain}, $K_k$, is an $n$-by-$m$ matrix expressed as:
\begin{align*}
K_k = P_k^-H_k^T(H_kP_k^-H_k^T + R_k)^{-1}
\end{align*}

Assuming no external control, the a priori estimate $x_k^-$ of the state is
given by:
\begin{align*}
x_k^- = Fx_{k - 1} + w_k,
\end{align*}
where $F$ is the $n$-by-$n$ \textit{transfer matrix} characterizing the
dynamics of the system, and $w_k$ is the \textit{process noise} associated with
random events or forces that directly affect the actual state of the system. We assume that the components of $w_k$
have Gaussian distribution $N(0, Q_k)$ for some $n$-by-$n$ covariance matrix
$Q_k$.

Using $P_k^-$ to denote the error covariance, the a priori estimate for this
covariance at time $k$ is obtained from the value at time $k - 1$ by:
\begin{align*}
P_k^- = FP_{k - 1}F^T + Q_{k - 1}
\end{align*}

The updated value for $x_k$ when a new measurement is available is:
\begin{align*}
x_k = x_k^- + K_k(z_k^- - H_kx_k^-)
\end{align*}
The update value for $P_k$ is:
\begin{align*}
P_k = (I - K_kH_k)P_k^-
\end{align*}

\section{Appendix B - Review of Dynamic Bayesian Network}
This part is mainly based on the Ph.D. thesis by Murphy \cite{murphy02}.

An HMM is a stochastic finite automaton, where each state generates (emits) an
observation. The transition model is usuasually characterized by a conditional
multinomial distribution: $A(i, j) = P(X_t = j | X_{t-1} = i)$, where $A$ is a
stochastic matrix.

\subsection{Filtering}
The most common inference problem in online analysis is to recursively estimate
the belief state using Bayes's rule:

\begin{align*}
P(X_t | y_{1:t}) & \propto P(y_t | X_t, y_{1:t-1})P(X_t | y_{1:t-1}) \\
 				 & = P(y_t | X_t) \left[\sum_{x_{t - 1}} 
				 	 P(X_t | x_{t - 1})P(x_{t - 1} | y_{1:t - 1})\right]	
\end{align*}

This task is traditionally called ``filtering'', because we are filtering out
the noise from the oberservations. We can find this value using the forward
algorithm.

\subsection{Classification}
The likelihood of a model, $M$, is $P(y_{1:t}|M)$. This can be used to classify
a sequence as follows:
\begin{align*}
C^*(y_{1:T}) = \arg \max_{C} P(y_{1:T} | C)P(C)
\end{align*}

\section{Appendix C - Review of Random Conditional Fields}
Each clique template $C_p$ is a set of factors which has a corresponding set
of sufficient statistics $\{f_{pk}(\mathbf{x_p}, \mathbf{y}_p)\}$ and
parameters $\theta_p\in\mathcal{R}^{K(p)}$. The CRF can be written as
\begin{align}
p(\mathbf{y}|\mathbf{x}) = \frac{1}{Z(\mathbf{x})}\prod_{C_p\in
\mathcal{C}}\prod_{\Psi_c\in C_p}\Psi_c(\mathbf{x}_c,
\mathbf{y}_c;\theta_p)
\end{align}
where each factor is parameterized as
\begin{align}
\Psi_c(\mathbf{x}_c, \mathbf{y}_c;\theta_p) =
\exp\left\{\sum_{k=1}^{K(p)}\lambda_{pk}f_{pk}(\mathbf{x}_c,
\mathbf{y}_c)\right\}
\end{align}
and the normalization function is 
\begin{align}
Z(\mathbf{x}) 
&= \sum_{\mathbf{y}}\prod_{C_p\in\mathcal{C}}\prod_{\Psi_c\in
C_p}\Psi_c(\mathbf{x_c}, \mathbf{y_c}; \theta_p)\\
&= \sum_{\mathbf{y}}\exp\left\{\sum_{C_p\in\mathcal{C}}\sum_{\Psi_c\in
C_p}\sum_{k=1}^{K(p)}\lambda_{pk}f_{pk}(\mathbf{x}_c,
\mathbf{y}_c)\right\}
\end{align}

The conditional log likelihood is given by 
\begin{align}
\ell(\theta) = \sum_{C_p\in\mathcal{C}}\sum_{\Psi_c\in
C_p}\sum_{k=1}^{K(p)}\lambda_{pk}f_{pk}(\mathbf{x}_c, \mathbf{y}_c) - \log Z(\mathbf{x})
\end{align}

The partial derivative of the log likelihood with respect to a parameter
$\lambda_{pk}$ associated with a clique template $C_p$ is
\begin{align}
\frac{\partial\ell}{\partial\lambda_{pk}} 
&= \sum_{\Psi_c\in C_p}\lambda_{pk}f_{pk}(\mathbf{x}_c, \mathbf{y}_c) -
\frac{1}{Z(\mathbf{x})}\frac{\partial Z(\mathbf{x})}{\partial\lambda_{pk}}\\
&= \sum_{\Psi_c\in C_p}\lambda_{pk}f_{pk}(\mathbf{x}_c, \mathbf{y}_c) -
\frac{1}{Z(\mathbf{x})}\sum_{\mathbf{y}}\exp\left\{\sum_{C_p\in\mathcal{C}}\sum_{\Psi_c\in
C_p}\sum_{k=1}^{K(p)}\lambda_{pk}f_{pk}(\mathbf{x}_c,
\mathbf{y}_c)\right\}\sum_{\Psi_c\in C_p}f_{pk}(\mathbf{x}_c, \mathbf{y}_c) \\
&= \sum_{\Psi_c\in C_p}\lambda_{pk}f_{pk}(\mathbf{x}_c, \mathbf{y}_c) -
\frac{1}{Z(\mathbf{x})}\sum_{\mathbf{y}}p(\mathbf{y}, \mathbf{x})\sum_{\Psi_c\in
C_p}f_{pk}(\mathbf{x}_c, \mathbf{y}_c) \\
&= \sum_{\Psi_c\in C_p}\lambda_{pk}f_{pk}(\mathbf{x}_c, \mathbf{y}_c) -
\frac{1}{Z(\mathbf{x})}\sum_{\Psi_c\in C_p}\sum_{\mathbf{y}}p(\mathbf{y},
\mathbf{x})f_{pk}(\mathbf{x}_c, \mathbf{y}_c) \\
&= \sum_{\Psi_c\in C_p}\lambda_{pk}f_{pk}(\mathbf{x}_c, \mathbf{y}_c) -
\frac{1}{Z(\mathbf{x})}\sum_{\Psi_c\in C_p}\sum_{\mathbf{y'_c}}p(\mathbf{y'_c},
\mathbf{x})f_{pk}(\mathbf{x}_c, \mathbf{y'}_c) \\
&= \sum_{\Psi_c\in C_p}\lambda_{pk}f_{pk}(\mathbf{x}_c, \mathbf{y}_c) -
\sum_{\Psi_c\in C_p}\sum_{\mathbf{y'_c}}f_{pk}(\mathbf{x}_c, \mathbf{y}_c)p(\mathbf{y'_c}
| \mathbf{x})
\end{align}
