\begin{savequote}
Language is a part of social behavior. What is the mechanism whereby the social
process goes on? It is the mechanism of gesture\ldots
\qauthor{George Herbert Mead, \textit{Mind, self, and society}}
\end{savequote}
\chapter{Conclusion}
I developed a real-time continuous gesture recognition system for natural
human computer interaction. The
unified probabilistic framework for handling two forms of gestures is a novel
approach, and the evaluation shows promising results: an
average online recognition $F_1$ score of 0.805 using the hybrid performance
metric, on a challenging dataset with unsegmented gestures of different forms.
The system also achieves real-time performance at 30FPS. Using the framework, I
developed a gesture controlled presentation application similar to the one described at the beginning of this paper. All the code is open-source
and is available
online\footnote{\url{http://groups.csail.mit.edu/mug/projects/gesture_kinect/index.html\#code}}.

Another novel approach is using embedded training and hidden state information
to detect gesture phases, allowing the system to respond more promptly. On
average, for discrete flow gestures, the system responds 0.6s before the hand comes to
rest. 

I collected a new dataset that includes two forms of gestures, a
combination currently lacking in the community, and plan to make it
public. I also proposed a hybrid performance metric
that is more relevant to real-time interaction and different types of gestures.

\section{Limitations}
Currently, the system handles only single-hand gestures. It will be great
to add support for two-hand gestures. One challenge for this is handling
cases where there is occlusion between the two hands. Once features from both
hands are computed, they can be concatenated, and the recognition module could
remain the same.

The number of hidden states per gesture needs to be specified by the user or the
application developer in the gesture definition file. In the future work, we
will make the system automatically learn the number of hidden states through
cross-validation and by grouping similar frames~\cite{song13}.

The performance for pose gestures is lower than that
for path gestures. Two factors contribute to this problem: the limitation in
both the pixel and the depth resolutions of the Kinect sensor, and motion blur.
It would be interesting to test with the new version of the Kinect sensor, which
uses a time-of-flight depth sensor and is reported to have a higher resolution. Other feature descriptors
(e.g., histograms of optic flows) and encoding methods (e.g, sparse
coding~\cite{lee07}) can be explored as well.

The performance of a user independent model is relatively low. This is expected
because there is large variation among users and the number of users in our
dataset is relatively small to train a general model.
Even though users can quickly define and train their own models, a relatively
accurate base model is always valuable. Hence, more work can be done to improve the base
model, e.g., using mixture of HMMs~\cite{keskin12}. The mixture of Gaussians
used in the current system accounts for variation in each hidden state, but it
cannot model variations in transition probabilities between the hidden states.
The mixture of HMMs can fill the gap in this regard.

\section{Future Work}
In addition to work that address the limitations, future research should also
consider pushing ahead on natural human computer interaction.

\textbf{Sensors} In this thesis, I considered the Kinect and the IMU as
input sensors. The gesture controlled presentation application currently
only uses the Kinect data. We can explore even more combinations of sensors
by including the Leap Motion Controller and smartwatches. Each sensor has
strengths of its own, so by combining them we can get the best system
overall. The Leap Motion Controller is used mostly as an environmental sensor,
but we can also explore its use case a wearable sensor (e.g., attaching it to
the arm or wearing it in front of the chest).
Its high accuracy in finger tracking may help improve recognition performance for pose gestures.

\textbf{User adaptation} This is a hot topic in both machine learning and
HCI, and is applicable to many domains such as speech recognition and
touch screen keyboard input~\cite{yin13-making}. Currently, the system
uses a binary decision to do user adaptation: if a user-trained model is
present, the system will use that model; otherwise it uses the base
model. It would be more convenient if the system learned the user
dependent model implicitly while the user is using the system, and used
soft decisions to combine the base model and user dependent model, e.g.,
weighted combination.

\textbf{Interactive training} Due to the large variation in the way users
performing gestures and differences in user preference, the gesture recognition
system would benefit from having an easy to use interface for fast training.
Many efforts in machine learning research assume the availability of
training data. Creating interfaces that are easy for users to provide training examples, either in a
separate session, or when using the system and provide feedback to the system
for wrong recognition, would be an interesting interdisciplinary topic in
machine learning and HCI.

\textbf{Context-aware gesture recognition} When I was using the gesture
controlled presentation application during a talk, I had to be careful 
not to do a gesture that the system would recognize and respond to 
even though that was not my intention. This kind of restriction can make users
feel more constrained and reduce the naturalness of the interaction. The reason
for this is that currently, whenever the system detects a movement that matches a
gesture model, it will respond. People are much better at discerning whether or
not a gesture is intended as a command, based on the context. It will be
interesting to add this capability to the system as well. For example, in the
gesture controlled presentation application, we might model the context based on
what the user is saying and the state of the application (e.g., the content on
the slide and the progression on the slide) and make gesture recognition
dependent on the context.
