\chapter{Related Work}\label{chap:related}
A gesture input
pipeline can be broken down into several sequential modules: sensors, hand
tracking, feature extraction, gesture recognition, and user interfaces.
I discuss related work in each module.

\section{Sensors}
The first step in the pipeline is having sensors capture hand movements
and configurations. Early attempts
to solve this problem resulted in mechanical devices that measure hand
joint angles and spatial positions directly. This group is best represented by
the glove-based approaches using devices such as CyberGloves \cite{fels09} and
Powergloves \cite{kadous02}. However, wearing gloves makes gesturing more
cumbersome, leading to many efforts to make the gloves more
light-weight (e.g., by using Bluetooth wireless data transmission as in the
CyberGove II). To further reduce the bulkiness of the gloves, people have used
colored markers on the fingers \cite{mistry09} or colored gloves with no
electronics \cite{Wang09} and using RGB cameras and computer vision techniques
to interpret gestures. However, by requiring the user to wear something extra hinders the acceptance of
such devices as ``everyday'' natural interaction interfaces. 

The most non-obtrusive way to capture the hand is bare-hand tracking. Shin et
al. \cite{shin04} use stereo RGB cameras to extract the hand from background
based on skin colors. One limitation of RGB cameras is that they are very
sensitive to lighting condition. This prompted researchers to look into other types of cameras. Oka et al. 
\cite{Oka02} use thermal imaging for hand segmentation under complex
background and changing light, relying on the condition that a hand's
temperature is almost always distinct from the background. 
Larson et al. \cite{larson11}
improved on this method by adding touch detection. They detect
finger contacts by using heat transferred from a user's hand to the
surface for touch-based gestures.
However, in order to detect the heat trace, users have to drag their fingers a
bit instead of just touching. This may be a small departure from what users
would expect as ``natural'' based on their experience in the physical world.

Thermal imaging measures radiation emitted by objects in the
far-infrared (F-IR) spectrum. There are other well-known ``IR-imaging''
techniques used in the HCI community which use devices operating in the
near-infrared (N-IR) spectrum.
N-IR is employed in some fairly recent interactive tabletop surfaces and depth
cameras~\cite{izadi08, zhang12}. A number of projects have used
IR for tabletop interaction by detecting multi-touch gestures using an
under-surface mounted camera and an illumination source, e.g.,
Microsoft's Surface. Recently, multi-touch phones and tablets have become more
and more ubiquitous. These devices are based on capacitive touch sensitive screens. Touch-based devices are
becoming more sophisticated, but are
still limited to gestures in 2D space with one or multiple
fingers. 

Depth sensing input devices such as the Kinect sensor and the Leap Motion
Controller open up the possibility of gesturing in 3D space. The Kinect sensor
contains a depth sensor, a color camera, and a four-microphone array that
provide full-body 3D skeleton tracking, face recognition, and voice
recognition capabilities~\cite{zhang12}. The first generation of the sensor
uses structured light for depth sensing.
The color stream has a resolution of $640\times
480$px at 30Hz or $1280\times960$px at 12Hz.  The depth sensing video stream has a resolution of $640\times 480$px at
30Hz\footnote{\url{http://msdn.microsoft.com/en-us/library/jj131033.aspx}}. 
With the default range, the Kinect sensor (version one) has a depth sensing
range limit of about 0.8m -
4m\footnote{\url{http://msdn.microsoft.com/en-us/library/hh973078.aspx}}.
The random error of its depth measurements increases quadratically with
increasing distance from the sensor, and ranges from a few millimeters at 0.5m
distance to about 4cm at the maximum range of 5m~\cite{khoshelham2011}. It
can track 20 body joints including hands. The newer version of Kinect uses a
time-of-flight camera for depth sensing, and a higher resolution color camera ($1920\times 1080$px at
30Hz)\footnote{\url{http://www.develop-online.net/news/next-xbox-leak-reveals-kinect-2-specs/0114096}}.
It can track 25 body joints (including
 thumbs)\footnote{\url{http://en.wikipedia.org/wiki/Kinect\#Kinect_for_Xbox_One}}.

In stead of full-body tracking, the Leap Motion Controller has a smaller observation area and specializes in hand tracking with higher
resolution.
It uses two monochromatic IR cameras and three infrared LEDs, and observes a
roughly hemispherical area, to a distance of about 1m~\cite{leapmotion14}. It
can track all 10 fingers up to 1/100th of a
millimeter\footnote{\url{https://www.leapmotion.com/product}}. However, as
the algorithm is optimized for detecting finger-shaped objects, detecting other
hand shapes such as ``palm up'' (with finger closed) or ``fist'' can be
challenging with the Leap Motion Controller.

More recently, a new generation of smart watches is leading the way in wearable
computing interfaces.
These smart watches are equipped with motion sensors such as
accelerometers, gyroscopes and magnetometers, which are basic components of an
inertial measurement
unit
(IMU). These sensors can give relatively accurate motion and orientation
information of users' hands at a high frame rate (e.g., the Xsens MTw IMU has
a frame rate of 50Hz~\cite{Ruffieux2013}), and can thus be used for
gesture input.
While data from camera-based sensors is prone to occlusion, and its quality
of accuracy is highly dependent on the position of the user relative to the
sensor, IMU sensors are occlusion free and position independent. In
addition, data from IMUs can be used with less complex processing
compared with data from camera-based sensors~\cite{Ruffieux2013}.The
disadvantage of an IMU is that it cannot capture hand shape information.

There is a plethora of new sensors, and they have both pros and cons for hand
tracking and gesture recognition. It is possible to combine different sensors
so that they can complement each other. Sensor technology will continue to
advance, and as a result it is important to make gesture recognition system
 flexible and generalizable to different feature input. 

I classify
sensors into two categories according to their placement:
environmental and wearable. An environmental sensor is installed at a fixed
position (e.g., the Kinect senor and the Leap Motion Controller); while a
wearable sensor is worn by the user (e.g., smart watches). I
evaluated my system with sensors from both of these categories: a Kinect
sensor and an IMU sensor.
The Kinect sensor's microphone array is
useful for multimodal interaction, while its depth sensor and color camera can
be used for bare hand tracking and hand pose recognition, integral
parts of my system. With
the IMU, I demonstrate that the framework is generalizable and the two sensors can be used together.

\section{Hand Tracking}
The next step in the pipeline is
tracking the hand(s), i.e., localizing and segmenting hands from the rest of
the input. This step is substantive for camera based sensors, but trivial
for sensors worn on hands or wrists. 

Common hand tracking methods are based on either skin detection~\cite{shin04} or
motion detection~\cite{cutler98}.
Skin detection in the HSV~\cite{bradski98} or the YCrCb color space can be
relatively robust and less sensitive to lighting conditions, compared to the RGB
color space. However materials with skin-like colors (such as clothes) can produce false positives. Skin from other parts of the body can also interfere with hand tracking. 
Shin et at.~\cite{shin04} filter out false positives by considering the skin blob closest to the camera. However this can fail when the hand is close to
the body, generally resulting in the face being detected instead. Methods based
on motion detection would assume the background is relatively static and
there is no motion from other parts of the body. 

Marcos-Ramiro et al.~\cite{marcos2013} developed a method of computing hand likelihood maps based on RGB videos. They mention that, given a frontal, static camera pointing to the gesturer, hands are usually the closest
part to the camera, and also move with the highest speed. These characteristics translate to more movement in the image where
the gesturing hand(s) is. As they only used non-stereo RGB images, they could
not compute the closeness of the movement.

Hand tracking can be considered as a feature localization/detection
problem. Feature detectors usually select spatio-temporal
locations and scales in video by maximizing specific
salience functions~\cite{wang-spatio-09}. One example is space-time interest
points (STIPs) detector introduced by Laptev~\cite{laptev03}. He used it for human
action recognition from movies~\cite{laptev08}.
STIPs are points in the video sequence where the image values have significant local variations in both space and time. His method extends the Harris interest point detector in the spatial domain~\cite{Harris88} to the spatial-time domain. For each cuboid of interest point, both histograms of oriented gradients (HOG) and histograms of optic flow (HOF) are computed as feature descriptors.
Although promising results were demonstrated using STIPS, Wang et
al.~\cite{wang-spatio-09} find that the simple dense sampling method
(i.e., computing feature descriptors at regular positions and scales in space
and time) outperforms STIPs and other interest point detectors for action
recognition in realistic video settings.
They suggest that this indicates the limitations of current interest point detectors.

Another approach to hand tracking is based on 3D body pose estimation and searching for the hand region near the wrist~\cite{song11}.
Skeleton tracking provided by the Kinect Software Development Kit (SDK) gives a relatively robust 3D body pose estimation. The tracking
is based on a body part detector trained from a random forest of a huge number
of synthetically-generated body poses~\cite{shotton13}. One of its major
strengths is that it does not require an initialization pose. 
However, the hand joint tracking
from the Kinect SDK fails when the hands are close to the body or are moving
fast, especially in the seated mode~\cite{yin13}.

With a coarse initialization of a hand's configuration, Sudderth et
al.~\cite{sudderth04} use a graphical model of hand kinematics and nonparametric
belief propagation (NBP) to refine the tracking. However, the process is
relatively slow (1 min per NBP iteration o a Pentium IV workstation and the
process requires multiple iterations) and cannot be used in real-time yet. 

My hand tracking method is salience based and is similar to Marcos-Ramiro
et al.'s, but I combine both RGB images and depth images to compute gesture salience. I use depth data to compute the
 amount of motion, which is computationally less expensive than the optical flow
 method they used. By combining skin, motion
and closeness together to compute a probability map for the gesturing hand
location, my method better filters out false positive regions with skin-like
colors and makes fewer restrictions on motion. I compared my method with the
dense sampling method using the same dataset and recognition method, and my
method gives 11.6\% absolute improvement in frame-based classification $F_1$
score. Compared with the hand joint tracking from Kinect SDK, my method is also
shown to be more robust when the user is seated and the hands are close to the body or are moving fast.

\section{Feature Extraction}
The extraction of low level image features depends on the hand model in use, and
the complexity of the hand model is, in turn,  application dependent.
For some applications, a very coarse and simple model may be sufficient. The
simplest model is treating the hand as a blob and tracking only the 2D/3D
location of the blob. For example, Sharma et al.~\cite{sharma00} use 2D position
and time difference parameters to track the hands as blobs, which is sufficient
for them to distinguish whether the hands are doing a point, a contour, or a circle gesture. The PrimeSense NITE
middleware for OpenNI's natural interaction framework also tracks the hand as a
point. It requires the user to do a ``focus'' gesture (``click'' or ``wave'')
in order to start hand tracking~\cite{primesense-manual}. The
gestures it supports are ``click'' (pushing hand forward), ``wave'', ``swipe
left'', ``swipe right'', and ``raise hand''.

However, to make a more generalizable system for natural interaction, a
more sophisticated model is required. One step forward is adding fingertip
locations in the hand model as in~\cite{Oka02, harrison11,
larson11}.
Tracking fingertips is usually sufficient for manipulating objects on the 2D surface. 
To support a richer set of gestures involving 3D manipulation, we may need a
more sophisticated model. For instance, Wang et al. \cite{Wang09} uses a 26 DOF
3D skeletal model in their real-time hand-tracking system with a color glove.
Oikonomidis et al.~\cite{oikonomidis11} also used a parametric 3D model with 26
DOF and 27 parameters. They treat hand pose estimation from markerless visual
observation as an optimization problem and achieved a 15Hz frame rate with
an average error of 5mm (distance between corresponding finger joints in the
ground truth and the in the estimated model) .

Another approach is using an appearance-based model. This means that the model
parameters are not directly derived from the 3D spatial description of the hand.
The hand poses are modeled by relating the appearance of any pose to the 
appearance of the set of predefined, template poses \cite{Pavlovic97}. In
their markless hand-tracking system, Wang et al. \cite{wang11} use efficient
queries of a database of gestures and desktop-specific hand silhouette samples
for pinch/click gesture detection.

In this work, I use a simplified 3D
skeletal hand model for horizontal display interaction where hands are close
to the display surface and can directly manipulate virtual objects.
For communicative gestures, we need to know just the meaning of the gesture
and do not require exact spatial parameters. Hence appearance-based models
is more suitable which also require less computation, allowing us to achieve
a real-time frame rate of 30Hz.

\section{Gesture Recognition}
Many previous efforts on gesture recognition have focused on a single category
of gestures (either path gestures or pose gestures), though here are some work that
addressed the problem of handling multiple gesture categories in one system.

\subsection{Pose Gestures}
One group of prior work focuses on classifying a set of predefined static hand
poses frame by frame. Freeman and Roth \cite{freeman95} use histogram of local
orientations, a precursor of HOG~\cite{dalal05},
for hand pose recognition.
Recognition is based on selecting the feature vector in the training set that is closest to the test feature vector. 
Suryanarayan et al.
\cite{suryanarayan2010} use a volumetric shape descriptor computed from depth
data as the feature vector, and use Support Vector Machine (SVM) for
classification.
I use HOG as a hand pose feature descriptor but incorporate it in a unified HMM-based framework for both pose and path gestures.

\subsection{Path Gestures}
Another group of prior work focus on recognizing only path gestures. 
Most of these efforts used a hidden Markov model (HMM) and its variants to
recognize such gestures~\cite{Starner95, sharma00}.

Starner and Pentland used HMMs to recognize the part of American Sign Language
that uses path gestures~\cite{Starner95}. They collected about 500 sentences of a specific grammar (``personal pronoun, verb, noun, adjective, (the same) personal pronoun'') with a total lexicon of
forty words. No intentional pauses were placed between signs within a
sentence, but the sentences themselves were distinct. Because finger spelling
was excluded and there were few ambiguities in the
vocabulary based on individual finger motion, each gesture word would have a
distinct path.
The feature vector they used includes 8 elements:
each hand's x and y position, angle of axis of least inertia, and eccentricity of bounding ellipse.
They use Gaussian distributions to model the emission probabilities.
When training the HMMs, they used Viterbi training (see Appendix~\ref{app:hmm}
for more details on HMMs) to estimate the initial means and variances of the
output probabilities (after initially dividing the evidence equally among the
words in the sentence).
The initial estimates are fed into a Baum-Welch re-estimator, whose estimates are refined in embedded training.
The techniques they use are very similar to the ones used in speech
recognition (not surprisingly given the close parallel between sign language
and speech).
In fact, they were able to use Entropic's Hidden Markov Model TookKit (HTK)\footnote{\url{http://htk.eng.cam.ac.uk/}} directly for all
the modeling and training tasks. 

Gestures for HCI usually are less structured than sign language and do not
follow a grammar, hence to learn context-dependent transition and emission parameters will require $O(N^2)$
training examples to cover all possible pairs of gestures for
$N$ gestures.
But our user study indicate that people prefer to give around 5 examples per
gesture (i.e. linear growth with the number of gestures). To prevent quadratic
growth of required training examples,  I do not embed gesture HMMs in a
sentence to learn context-dependent models for inter-gesture transition and
assume the transitions between gestures have equal probability. However, I do
embed gesture \textit{phase} HMMs in a full gesture sequence to identify
different gesture phases which is necessary for real-time interaction.
In this way, we learn context-dependent models within a gesture, i.e.,
different gestures may have different pre-stroke and post-stroke phases
depending on their starting and ending points. 

Sharma et al.~\cite{sharma00} considered natural gestures that do
have grammar constraints.
They examined hand gestures made in a natural domain, weather narration, and
identified three deictic gestures:
\textit{point}, \textit{area}, and \textit{contour}. They also
considered the pre-stroke and post-stroke gesture phases, and used 5 states for
each of the pre-stroke, post-stroke and point HMMs and 6 states for each of the
contour and rest HMMs. The feature vector they use is motion based, and
includes relative distance between the hand and the face, angle of the arm,
angular velocity, radial velocity and tangential velocity. Hand poses are not
considered.
Without considering speech input, they obtained 69.52\% recognition accuarcy for the three gestures.
Although the deictic gesture is an important category in HCI, it is still too
limited to support a broader range of applications.

More recently, discriminative models such as
conditional random fields (CRF) and its variants, such as hidden CRF
\cite{wang06} and latent dynamic CRF (LDCRF) \cite{morency07}, have been
applied to gesture recognition with improved recognition results. Morency et al.
\cite{morency07} formulated the LDCRF model that is able to perform sequence labeling and segmentation simultaneously. 
Song et al.~\cite{song12} use LDCRF to distinguish path gestures where
certain gestures share the same paths but different hand poses. They use the HOG
feature descriptor for hand poses and use SVM to classify a pre-determined
set of hand poses. The result of the classification is used as part of the final
feature vector. They considered hand poses for gestures with distinct paths, but
did not handle arbitrary movement, as I am doing.

As discriminative classifiers model the posterior $p(y|x)$ directly, or
learn a direct map from inputs $x$ to the class labels $y$, it is
generally believed that discriminative classifiers are preferred to
generative ones (such as HMMs) for classification problems.
However, one limitation of CRF-based models is that training for these models
is much more expensive computationally and converges much slower than those of
HMMs \cite{lafferty01} because of the larger parameter space to search.
Discriminative models can also require more training data to reach
lower asymptotic error rates~\cite{ng02}. I applied LDCRF for gesture
classification and gesture phase segmentation.
Compared with my concatenated HMM-based method, LDCRF gives better gesture phase
segmentation result, but lower gesture classification result. The LDCRF model
also takes much longer time to train (18hr on the ChAirGest dataset) compared to
the HMM-based method (which takes 7min). As my user study shows that people
prefer to use a few examples to define their own gestures when necessary (see
Section~\ref{sec:preferences}), I decided to use the HMM-based model, which is
fast to train and requires fewer training examples.

\subsection{Multiple Categories of Gestures}
Some work has addressed the problem of handling multiple gesture
categories in one system. Keskin et al. \cite{keskin12} propose a unified
framework to allow concurrent usage of hand gestures, shapes and hand skeletons. 
Hand gestures are modeled with mixture of HMMs using spectral clustering. Hand shape classification and
hand skeleton estimation are based on classifying depth image pixels
using randomized decision forests.
Hand gesture classification is active all the time. The framework estimates a set of
posteriors for the hand shape label at each frame, and continuously uses these
posteriors and the velocity vector as observations to spot and classify known
gestures. They distinguish gestures with pure motion and pure hand shape by
thresholding the magnitude of the velocity vector. However, they did not mention
handling gestures that combine distinct hand poses with arbitrary movement.
For this category of gesture, it will be hard to manually set a velocity
threshold to distinguish them from gestures with distinct paths.

Oka et al. \cite{Oka02} developed a system that
allows both direct manipulation and symbolic gestures. These two categories
are in the \textit{nature} dimension. Based on the tracking result, the system
first differentiates the gesture as either manipulative or symbolic according to
the extension of the thumb. They define gestures with an extended thumb as
direct manipulation and those with a bent thumb as symbolic gestures.
For direct manipulation, the system selects operating modes such as rotate, 
move or re-size based on the fingertips configuration; for symbolic gestures, it 
uses HMMs for classification. The use of thumb to distinguish
manipulative and communicative gestures seems arbitrary and unnatural. 
My system does not
require an arbitrary hand pose to indicate gesture categories. It handles the
path and pose gestures seamlessly under one recognition framework. I believe
that this can help to reduce users' cognitive load for using the interface, and
make the interaction more natural.

\subsection{Gesture Spotting}
A common approach to distinguish non-gestures from gestures is to use one or two
non-gesture HMMs to provide likelihood thresholds for outlier rejection~\cite{yang07}. 
Peng et al. \cite{peng11} argue that using one or two HMMs cannot
effectively reject non-gesture outliers that resemble portions of
gestures. In addition to a general non-gesture model, they train several
non-gesture HMMs by automatically identifying and manually specifying
non-gesture models from the training data. This approach is suitable when the
set of the input data is limited, but in real life the set
of possible non-gestures is limitless and it is not clear how this
approach will scale.

I use gesture phases to distinguish gestures from non-gestures. As explained in
Section~\ref{sec:temporal-model}, every gesture must have a nucleus phase.
My system identifies different gesture phases, and any hand movement that does
not contain a nucleus phase will be treated as a non-gesture. I combine this
method with a thresholding method to improve the overall performance of gesture
spotting.

\subsection{Online Recognition}
Another group of work focus on online gesture recognition which is important
for real-time interactive systems. Song et al.
\cite{song12} extend LDCRF with multi-layered filtering and a temporal sliding window to perform online sequence labeling and
segmentation simultaneously.
Their approach incurs one to four seconds delay in their
experiments. For real-time interaction, 0.1s is about the limit for having the
user feel that the system is reacting instantaneously. We may relax this response time a
bit, but 1.0s is about the limit for the user's flow of thought to stay
uninterrupted \cite{card91}.  Song et al. focus only on communicative
gestures and assume no non-gestures in the input data. 

Fran{\c{c}}oise et al. \cite{francoise11} use hierarchical HMMs to model musical
gestures using motion data from the Wii remote controller, and use fixed-lag
smoothing for real-time recognition and segmentation.

My method is similar to Fran{\c{c}}oise et al.'s, but I consider path and pose
gestures in a single framework. My system incurs a 0.3s delay, which is shorter
than that of Song et al.'s method.

\subsection{Commercial Systems}
There are several commercially availabel gesture recognition systems that
are worth noting. 
The gesture interaction provided by the Kinect-based games is
one of the popular ones. Based on the observation of the interaction
available in Kinect games, it seems that the system is looking for only one
gesture (wave) or body pose (the exit
pose\footnote{\url{http://support.xbox.com/en-US/xbox-360/kinect/how-to-use-the-kinect-hub-and-guide}}) at a time, and the rest of the time, 
it is just tracking the hand and the body, and turns the hand into a mouse.

Many commercial gesture recognition systems use if-then rules
based on heuristics. For example, the Leap Motion plugin\footnote{\url{https://github.com/hakimel/reveal.js/blob/master/plugin/leap/leap.js}}
for the Reveal.js\footnote{\url{http://lab.hakim.se/reveal-js}} HTML5
presentation framework uses the number of fingers detected and the changes in the $x$ and $y$ coordinates between consecutive
frames to detect swipe and point gestures. While if-then rules could be easy to define for
a small number of simple gestures, they may be hard to define for more complex
gestures.
For example, it may be hard to define a circle gesture using if-then rules
because a sequence of several coordinate locations (instead of just two) are
needed, and the rules can conflict with other rules (e.g., the rules for swipe
left/right gestures).

% \begin{lstlisting}
% void LookForGesture() {
%   // Swipe to right
%   if (ScanPositions((p1, p2) => Math.Abs(p2.Y - p1.Y) < SwipeMaximalHeight, // Height
%       (p1, p2) => p2.X - p1.X > -0.01f, // Progression to right
%       (p1, p2) => Math.Abs(p2.X - p1.X) > SwipeMinimalLength, // Length
%       SwipeMininalDuration, SwipeMaximalDuration)) // Duration {
%     RaiseGestureDetected("SwipeToRight");
%     return;
%   }
% 
%   // Swipe to left
%   if (ScanPositions((p1, p2) => Math.Abs(p2.Y - p1.Y) < SwipeMaximalHeight,  // Height
%       (p1, p2) => p2.X - p1.X < 0.01f, // Progression to right
%       (p1, p2) => Math.Abs(p2.X - p1.X) > SwipeMinimalLength, // Length
%       SwipeMininalDuration, SwipeMaximalDuration))// Duration
%         {
%     RaiseGestureDetected("SwipeToLeft");
%     return;
%   }
% }
% \end{lstlisting}

\section{Multimodal User Interfaces}
Bolt's pioneering work in the ``Put That There'' system \cite{Bolt80} 
demonstrated the potential for voice and gestural interaction.  In that system, 
the hand position and orientation were tracked by the Polhemus tracker, i.e.,
the hand was essentially transformed to a point on the screen. The actual hand 
posture did not matter, even if it was not in a pointing shape. The speech also 
followed a rigid and limited command-like grammar. Even though this is early 
work, it provides some insight about the advantages of multi-modal interaction. 
As Bolt summarized in the paper, using pointing gesture allows the use of 
pronouns in the speech, with the corresponding gain in naturalness and economy 
of expression \cite{Bolt80}.

Since then, several multi-modal interaction prototypes have been
developed that moved beyond Bolt's ``Put That There'' system. Cohen et al. 
\cite{Cohen97} developed the QuickSet prototype, a collaborative, 
multi-modal system running on a hand-held PC using pen and voice as input. They 
used a multi-modal integration strategy that allows speech and pen gesture 
to compensate for each other, yielding a more robust system. Rauschert et al. 
\cite{Rauschert02} developed a system called Dialogue-Assisted Visual 
Environment for Geoinformation (DAVE\_G) that uses free hand gestures and speech
as input. They recognized that gestures are more useful for expressing spatial 
relations and locations. Gestures in DAVE\_G include pointing, indicating an 
area and outlining contours. Speech and gesture are fused for commands that need
spatial information provided by the gesture. 

In this thesis, I identify two ways of combining speech and gestures that are
particularly effective for HCI.
In addition to using deictic gestures to provide spatial information as a
complement to speech as in \cite{Rauschert02}, I also use speech as a
complement to manipulative gestures.
Based on the user study~\cite{yin10} I
conducted, I observe that manipulative gestures are at times accompanied by
adjectives and adverbs that refine the actions. I demonstrate these two
combinations in a gesture controlled presentation framework I developed.
