\chapter{Review of Conditional Random Fields}\label{app:crf}
Each clique template $C_p$ is a set of factors which has a corresponding set
of sufficient statistics $\{f_{pk}(\mathbf{x_p}, \mathbf{y}_p)\}$ and
parameters $\theta_p\in\mathcal{R}^{K(p)}$. The CRF can be written as
\begin{align}
p(\mathbf{y}|\mathbf{x}) = \frac{1}{Z(\mathbf{x})}\prod_{C_p\in
\mathcal{C}}\prod_{\Psi_c\in C_p}\Psi_c(\mathbf{x}_c,
\mathbf{y}_c;\theta_p)
\end{align}
where each factor is parameterized as
\begin{align}
\Psi_c(\mathbf{x}_c, \mathbf{y}_c;\theta_p) =
\exp\left\{\sum_{k=1}^{K(p)}\lambda_{pk}f_{pk}(\mathbf{x}_c,
\mathbf{y}_c)\right\}
\end{align}
and the normalization function is
\begin{align}
Z(\mathbf{x})
&= \sum_{\mathbf{y}}\prod_{C_p\in\mathcal{C}}\prod_{\Psi_c\in
C_p}\Psi_c(\mathbf{x_c}, \mathbf{y_c}; \theta_p)\\
&= \sum_{\mathbf{y}}\exp\left\{\sum_{C_p\in\mathcal{C}}\sum_{\Psi_c\in
C_p}\sum_{k=1}^{K(p)}\lambda_{pk}f_{pk}(\mathbf{x}_c,
\mathbf{y}_c)\right\}
\end{align}

The conditional log likelihood is given by
\begin{align}
\ell(\theta) = \sum_{C_p\in\mathcal{C}}\sum_{\Psi_c\in
C_p}\sum_{k=1}^{K(p)}\lambda_{pk}f_{pk}(\mathbf{x}_c, \mathbf{y}_c) - \log Z(\mathbf{x})
\end{align}

The partial derivative of the log likelihood with respect to a parameter
$\lambda_{pk}$ associated with a clique template $C_p$ is
\begin{align}
\frac{\partial\ell}{\partial\lambda_{pk}}
&= \sum_{\Psi_c\in C_p}\lambda_{pk}f_{pk}(\mathbf{x}_c, \mathbf{y}_c) -
\frac{1}{Z(\mathbf{x})}\frac{\partial Z(\mathbf{x})}{\partial\lambda_{pk}}\\
&= \sum_{\Psi_c\in C_p}\lambda_{pk}f_{pk}(\mathbf{x}_c, \mathbf{y}_c) -
\frac{1}{Z(\mathbf{x})}\sum_{\mathbf{y}}\exp\left\{\sum_{C_p\in\mathcal{C}}\sum_{\Psi_c\in
C_p}\sum_{k=1}^{K(p)}\lambda_{pk}f_{pk}(\mathbf{x}_c,
\mathbf{y}_c)\right\}\sum_{\Psi_c\in C_p}f_{pk}(\mathbf{x}_c, \mathbf{y}_c) \\
&= \sum_{\Psi_c\in C_p}\lambda_{pk}f_{pk}(\mathbf{x}_c, \mathbf{y}_c) -
\frac{1}{Z(\mathbf{x})}\sum_{\mathbf{y}}p(\mathbf{y}, \mathbf{x})\sum_{\Psi_c\in
C_p}f_{pk}(\mathbf{x}_c, \mathbf{y}_c) \\
&= \sum_{\Psi_c\in C_p}\lambda_{pk}f_{pk}(\mathbf{x}_c, \mathbf{y}_c) -
\frac{1}{Z(\mathbf{x})}\sum_{\Psi_c\in C_p}\sum_{\mathbf{y}}p(\mathbf{y},
\mathbf{x})f_{pk}(\mathbf{x}_c, \mathbf{y}_c) \\
&= \sum_{\Psi_c\in C_p}\lambda_{pk}f_{pk}(\mathbf{x}_c, \mathbf{y}_c) -
\frac{1}{Z(\mathbf{x})}\sum_{\Psi_c\in C_p}\sum_{\mathbf{y'_c}}p(\mathbf{y'_c},
\mathbf{x})f_{pk}(\mathbf{x}_c, \mathbf{y'}_c) \\
&= \sum_{\Psi_c\in C_p}\lambda_{pk}f_{pk}(\mathbf{x}_c, \mathbf{y}_c) -
\sum_{\Psi_c\in C_p}\sum_{\mathbf{y'_c}}f_{pk}(\mathbf{x}_c, \mathbf{y}_c)p(\mathbf{y'_c}
| \mathbf{x})
\end{align}

\section{Linear-Chain CRF}\label{sec:linear-crf}
Each feature function has the form $f_k(y_t, y_{t-1},
\underline{x}_t)$.
We have one feature $f_{ij}(y, y', \underline{x}) =
\mathbbm{1}(y=i)\mathbbm{1}(y'=j)$ for each transition $(i, j)$ and one feature
$f_i(y, y', \underline{x}) = \underline{x}$ for each state.
Let $M$ be the total number of hidden states, and $D$ be the dimension of the
observation $\underline{x}$. The number of model parameters is $O(M^2 + DM)$.
The total training cost is $O(TM^2)$ where $T$ is sequence length, N is the
number of training examples and G is the number of gradient computations
required by the optimization procedure. Hence if the number of states is large,
the computation can be expensive~\cite{sutton06}.

The forward-backward algorithm for linear-chain CRF is identical to the HMM
version except that the transition weights $\Psi_t(j, i, \underline{x}_t)$ are
defined differently.

The Viterbi recursion for linear-chain CRF can be computed similarly as in HMMs
as well
\begin{align*}
\delta_t(s) = \max_{s'\in \mathcal{S}}\Psi_t(s, s',
\underline{x}_t)\delta_{t-1}(s')
\end{align*}

Like in HMMs, the vectors $\alpha_t$, $\beta_t$ and $\delta_t$ are normalized to
prevent underflow.
