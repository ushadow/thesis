\chapter{Datasets}

This chapter describes the datasets used to perform the experiments in this
dissertation.

\section{YANG Dataset}
Previous related work do not appear to have gesture data sets
that include both gestures with distinct paths and gestures with distinct hand
poses. To evaluate our method, we collected a new dataset named YANG (dichonomY
of in-Air Natural Gestures).

with a vocabulary of 7
one-hand/arm gestures focusing on combining these two forms of gestures. They
are also chosen to span over different potential difficulties (see the comments in Table~\ref{tab:gestures}).

\begin{table}
\caption{List of gestures recorded in the dataset.}
\label{tab:gestures}
\centering
\begin{tabular}{|c|l|l|l|}
\hline
\# & Name of gesture & Form & Comment \\
\hline
1 & Swipe left & distinct path & simple path \\
\hline
2 & Swipe right & distinct path & simple path \\
\hline
3 & Circle & distinct path & complex path \\
\hline
4 & Horizontal wave & distinct path & has arbitrary repetitions \\
\hline
5 & Point & distinct hand pose & arbitrary path \\
\hline
6 & Palm forward & distinct hand pose & arbitrary path \\
\hline
7 & Grab & distinct hand pose & arbitrary path \\
\hline
\end{tabular}
\end{table}

The dataset contains data from 10 participants each
doing 4 sessions. All the participants are university students.
The participants were shown video demonstration of each gesture at the beginning. 

\subsection{Recording Setup and Procedure}
In each session, the participant stands at about 1.5m from the Kinect
for Windows sensor (version one), and performs each gesture 3 times
according to the text prompts on a screen indicating the name of the gesture to
perform.
The order of the gestures is random and the time between each gesture is random
(between 2s and 6s). The first 2 sessions have ``Rest'' prompts between each
gesture, telling participants to go to the rest position (hands relaxing at the
side of the body), and the second 2 sessions do not have ``Rest'' prompts so
participants can choose to rest or not between consecutive gestures. This too
distinguishes the dataset from previous ones \cite{Ruffieux2013, guyon13} where
gestures are always delimited by rest positions.

Unlike Ruffieux et al. \cite{Ruffieux2013}, we do not show video demonstration
every time the participants perform a gesture because we want a
realistic scenario. In real practice, it is unlikely that a user will follow a
video demonstration every time he/she does a gesture. The result of this is that
there will be more variations among the gestures.

To motivate movement for gestures with distinct hand poses that
require a continuous response, the text prompt asks participants to draw
random letters in the air with the specified hand pose. 

The full corpus contains $
10P \times 4S \times 7G \times 3R = 840$ gesture occurrences
where P = participants, S = sessions, G = unique gestures, R = repetitions per
gesture. There are approximately 96 minutes of continuous recording,
recorded in the raw data from the Kinect sensor, including RGB, depth and
skeleton data. Both the RGB and depth data have a resolution of
$640\times480$px. The frame rate is 30 frame per second (FPS).

\subsection{Qualitative Observations}
We find that there is considerable variations in the way participants perform
each gesture even they were given the same demonstration video. Major variations
are observed in speed, the magnitude of motion, the paths and hand poses.

For example, some participants do swipe left and right in a rather straight
horizontal line, while others have a more curved path.  Some
participants do swipe left and right with a palm forward pose while others have
less distinct hand poses (hand is more relaxed). Some participants start
the circle gesture at the bottom, while others start at the top. Some
participants do the ``circle'' gesture in clockwise direction while others do it
in anti-clockwise direction.

However, within users, they are
quite consistent within each gesture. 

Image examples.

\subsection{User Preferences}
We did a survey with the participants with questions that can influence
gesture interface design. We asked them, given a gesture input interface:
\begin{itemize}
  \item User differences: As an example to show user differences, we asked
  the participants how they would prefer to do the ``circe'' gesture. 54\%
  prefer doing ``circle'' gesture in clockwise direction, 15\% in anti-clockwise
  direction, and 31\% do not care.
  \item Whether they prefer predefined gestures or user defined gestures: 90\%
  of the participants prefer to be able to define their own gestures if necessary while 10\% of them prefer to following prefined
gestures completely. As no one prefers to define their own gestures at the very
beginning either.
\item How to define gestures: 80\% prefer defining gesture by 
performing the gestures themselves; no one prefers to
define gestures solely via rules written in terms of positions and directions
of movement of the hands.
However 20\% prefer to being able to do both.
\item Number of repetitions per gesture for training: 50\% are willing to give a
maximum of 4 - 6 examples, 40\% are willing ot give 1 - 3 examples, and 10\% are
willing to give more than 13 examples. So average maximum is about 5
repetitions.
\item Number of gestures for an application: 80\% think 6-10 gestures are
appropriate and easy to remember for a given application, 20\% prefers 1 - 5
gestures.
\item Intuitiveness of the gesture vocabulary for PowerPoint presentation:
average score is 4 out of 5 where 5 is very intuitive.
\end{itemize}

\subsection{Implications for Gesture Interaction Interface}
Based on our observation of the large variation in gesture execution among
users and small variations within users, and the fact that a majority of
participants preferring defining their own gestures if they do not like the
predefined gestures, we suggest that it may be more important to optimize user
dependent recognition. As no one prefers to define their own gesture at the very
beginning, it also means that having a reasonable predefined gesture set and
basic user independent model for recognition will be useful too.

Recognition methods based on examples will allow users to train models of their
own gestures easily. We also need to develop methods that
require relatively few training examples and fast training speed.

\section{ChAirGest Dataset}
This is a publicly available dataset from the ChAirGest
challenge~\cite{Ruffieux2013}. It has data from both Kinect sensors and IMU
sensors, ground truth labeling of pre-stroke, nucleus and post-stroke phases. 
