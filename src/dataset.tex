\chapter{Datasets}

This chapter describes the datasets used to perform the experiments in this
dissertation.

\section{YANG Dataset}
Previous related work do not appear to have gesture data sets
that include both gestures with distinct paths and gestures with distinct hand
poses. To evaluate our method, we collected a new dataset named YANG (dichonomY
of in-Air Natural Gestures).

with a vocabulary of 7
one-hand/arm gestures focusing on combining these two forms of gestures. They
are also chosen to span over different potential difficulties (see the comments in Table~\ref{tab:gestures}).

\begin{table}
\caption{List of gestures recorded in the dataset.}
\label{tab:gestures}
\centering
\begin{tabular}{|c|l|l|l|}
\hline
\# & Name of gesture & Form & Comment \\
\hline
1 & Swipe left & distinct path & simple path \\
\hline
2 & Swipe right & distinct path & simple path \\
\hline
3 & Circle & distinct path & complex path \\
\hline
4 & Horizontal wave & distinct path & has arbitrary repetitions \\
\hline
5 & Point & distinct hand pose & arbitrary path \\
\hline
6 & Palm forward & distinct hand pose & arbitrary path \\
\hline
7 & Grab & distinct hand pose & arbitrary path \\
\hline
\end{tabular}
\end{table}

The dataset contains data from 10 participants each
doing 4 sessions. All the participants are university students.
The participants were shown video demonstration of each gesture at the beginning. 

\subsection{Recording Setup and Procedure}
In each session, the participant stands at about 1.5m from the Kinect
for Windows sensor (version one), and performs each gesture 3 times
according to the text prompts on a screen indicating the name of the gesture to
perform.
The order of the gestures is random and the time between each gesture is random
(between 2s and 6s). The first 2 sessions have ``Rest'' prompts between each
gesture, telling participants to go to the rest position (hands relaxing at the
side of the body), and the second 2 sessions do not have ``Rest'' prompts so
participants can choose to rest or not between consecutive gestures. This too
distinguishes the dataset from previous ones \cite{Ruffieux2013, guyon13} where
gestures are always delimited by rest positions.

Unlike Ruffieux et al. \cite{Ruffieux2013}, we do not show video demonstration
every time the participants perform a gesture because we want a
realistic scenario. In real practice, it is unlikely that a user will follow a
video demonstration every time he/she does a gesture. The result of this is that
there will be more variations among the gestures.

To motivate movement for gestures with distinct hand poses that
require a continuous response, the text prompt asks participants to draw
random letters in the air with the specified hand pose. 

The full corpus contains $
10P \times 4S \times 7G \times 3R = 840$ gesture occurrences
where P = participants, S = sessions, G = unique gestures, R = repetitions per
gesture. There are approximately 96 minutes of continuous recording,
recorded in the raw data from the Kinect sensor, including RGB, depth and
skeleton data. Both the RGB and depth data have a resolution of
$640\times480$px. The frame rate is 30 frame per second (FPS).

\subsection{Qualitative Observations}
We find that there is considerable variations in the way participants perform
each gesture even they were given the same demonstration video. Major variations
are observed in speed, the magnitude of motion, the paths and hand poses.

For example, some participants do swipe left and right in a rather straight
horizontal line, while others have a more curved path. Some participants start
the circle gesture at the bottom, while others start at the top. Some
participants do swipe left and right with a palm forward pose while others have
less distinct hand poses (hand is more relaxed). However, within users, they are
quite consistent within each gesture. 

\subsection{User Preferences}
We did a survey with the participants with questions that can influence
gesture interface design. We asked them, given a gesture input interface:
\begin{itemize}
  \item Whether they prefer predefined gestures or user defined gestures: 90\%
  of the participants prefer to be able to define their own gestures if necessary while 10\% of them prefer to following prefined
gestures completely. As no one prefers to define their own gestures at the very
beginning either.
\item How to define gestures: 80\% prefer defining gesture by 
performing the gestures themselves; no one prefers to
define gestures solely via rules written in terms of positions and directions
of movement of the hands.
However 20\% prefer to being able to do both.
\item Number of repetitions per gesture for training: 50\% are willing to give a
maximum of 4 - 6 examples, 40\% are willing ot give 1 - 3 examples, and 10\% are
willing to give more than 13 examples. So average maximum is about 5
repetitions.
\item Number of gestures for an application: 80\% think 6-10 gestures are
appropriate and easy to remember for a given application, 20\% prefers 1 - 5
gestures.
\item Intuitiveness of the gesture vocabulary for PowerPoint presentation:
average score is 4 out of 5 where 5 is very intuitive.
\end{itemize}

\subsection{Implications for Gesture Interaction Interface}
Based on our observation of the large variation in gesture execution among
users and small variations within users, and the fact that a majority of
participants preferring defining their own gestures if they do not the
predefined gestures, we suggest that it may be more important to optimize user
dependent recognition. As no one prefers to define their own gesture at the very
beginning, it also means that having a reasonable predefined gesture set and
basic user independent model for recognition will be useful too.

Recognition methods based on examples will allow users to train models of their
own gestures easily. We also need to develop methods that
require relatively few training examples.

\section{Hybrid Performance Metric}
Both frame \cite{song12} and event-based \cite{guyon13} metrics has been
used for evaluating gesture recognition systems. A \textit{frame} is a
fixed-length, fixed-rate unit of time. It is often the smallest unit of measure
defined by the system \cite{ward11} and in such cases approximates continuous
time.
For example, in our case, a frame is a data frame consisting of a RGB data frame, a depth data frame and a skeleton data from the
sensor at 30 FPS. An \textit{event} is a variable duration sequence of frames
within a continuous time-series.  It has a start time and a stop time.

Each of these metrics alone may not be adequate for evaluating a real-time
gesture recognition system handling different types of gestures. Frame-based
evaluation is less relevant for gestures requiring discrete responses. For the ChaLearn Gesture Challenge 2012, Guyon et al.
\cite{guyon13} use the Levenshtein distance between the ordered list of
recognized events and the ground truth events. However, such event-based metrics
that ignore timing of recognition are inappropriate for real-time
applications where responsiveness of the system matters.

Ruffieux et al. combined a time-based metric with an event-based metirc
\cite{Ruffieux2013}. Ward et al. \cite{ward11} proposed a comprehensive
scheme of combining both frame and event scoring.
We believe that all three types of information -- frames, events and
timings -- are relevant for a real-time system that responds to different types
of gestures. Hence, we developed a hybrid performance metric.

For discrete flow gestures, the system responds at the end of the nucleus phase,
so the evaluation should be event-based. Let $T_{{\text{gt\_start\_pre}}}$ be
the ground truth start time of the pre-stroke phase and
$T_{{\text{gt\_stop\_post}}}$ be the ground truth stop time of the post-stroke
phase.
A recognized event is considered a true positive (TP) if the time of response ($T_{\text{response}}$) 
occurs between $T_{{\text{gt\_start\_pre}}}$ and $T_{{\text{gt\_stop\_post}}} +
0.5\times(T_{{\text{gt\_stop\_post}}} - T_{\text{gt\_start\_pre}})$. We allow
some margin for error because there can be small ground truth timing errors.
Once a TP event is detected, the corresponding ground truth event is not
considered for further matching, so that multiple responses for the same gesture
will be penalized. We then can compute event-based precision, recall and F1 score.

For discrete flow gestures, we also define a Responsiveness Score (RS) as the
time difference in seconds between the moment when the system responds and the moment when the hand goes to a rest
position or changes gesture. Let $N_{TP}$ be the number of true positives, then
\begin{align}
RS = \frac{\sum_{i = 1}^{N_{TP}}T_{{\text{gt\_stop\_post}}} -
T_{\text{response}}}{N_{TP}}
\end{align}
A positive score means the responses are before the end of the post-strokes,
hence higher scores are better.

For continuous flow gestures, the system responds frame by frame,
so it is more appropriate to use frame-based evaluation. For all the frames that are
recognized as continuous gestures, we can compute the number of TPs by
comparing with the corresponding frame in the ground truth. Then, we can
compute frame-based precision, recall and F1 score for all the frames
corresponding to continuous flow gestures.

The average of the two F1 scores can give an overall indication of the
performance of the system.

