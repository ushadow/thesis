\begin{savequote}[120mm]
Each gesture is created at the moment of speaking and highlights what is
relevant and the same entity can be referred to by gestures that have changed
their form.
\qauthor{David McNeill, \textit{Hand and mind: what gestures reveal about
thought}}
\end{savequote}
\chapter{Gesture Interaction}
\section{Continuous and Discrete Gestures}
How to handle continuous and discrete gestures in an unified probabilistic
framework. Use hand poses to distinguish continuous and discrete gestures.
Unlike \cite{Oka02} et al., hand poses are selected based on naturalness.

\section{Gesture and Speech}
Traditionally, gestures are used to augment speech in interaction, as pioneered
by Bolt's ``Put That There'' system \cite{Bolt80}. Our previous user study
\cite{yin10thesis} show that speech also can augment gestures.

We use an off-the-shelf speech 
recognizer, and investigate how to combine both speech and gesture to make
the interaction more natural and effortless. Specifically, we will explore how speech and gestures complement each other in certain scenarios.

Two types of events: gesture and speech.

\begin{lstlisting}[caption=Gesture event JSON object]
{"gesture": <gestureName>, 
 "type": <"StartPreStroke" | "StartGesture" | "StartPostStroke">,
 "stage": <"PreStroke" | "Gesture" | "PostStroke">,
 "rightX": <rightHandXWorldCoordinate>,
 "rightY": <rightHandYWorldCoordinate> } 
\end{lstlisting}

\begin{lstlisting}[caption=Speech event JSON object]
{"speech": <speechTag>}
\end{lstlisting}

Number of hidden states for gestures with dynamic paths should at lest be 3.

\section{Gesture Controlled Presentation}
Server client architecture

Multimodal input server sends recognized gesture and speech events over a
WebSocket\footnote{http://en.wikipedia.org/wiki/WebSocket}. Any client
application can subscribe to the input events by connecting to the server
socket. The events are serialized in JSON format\footnote{http://www.json.org/}. 
