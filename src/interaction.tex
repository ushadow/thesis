\begin{savequote}[120mm]
Each gesture is created at the moment of speaking and highlights what is
relevant and the same entity can be referred to by gestures that have changed
their form.
\qauthor{David McNeill, \textit{Hand and mind: what gestures reveal about
thought}}
\end{savequote}
\chapter{Gestural Interaction}
This chapter discusses application level considerations for natural multimodal
interaction.

\section{Client Server Architecture}
I developed a multimodal input recognition engine that consists of a gesture
recognition engine and a speech recognition engine. The gesture recognition
engine includes the hand tracking, feature extraction and gesture recognition
modules described in the previous chapters.
For speech recognition, I used the application programming interface (API) from
the Windows for Kinect SDK\footnote{\url{http://msdn.microsoft.com/en-us/library/jj131035.aspx}}.

The multimodal input recognition engine runs as a server sending recognized
gesture and speech events over a WebSocket\footnote{http://en.wikipedia.org/wiki/WebSocket}. Any client
application can subscribe to the input events by connecting to the server
socket. There are two types of events: gesture and speech, and they are
serialized in JSON format\footnote{\url{http://www.json.org/}}
(Listing~\ref{lst:gesture} and~\ref{lst:speech}).

\begin{lstlisting}[caption={Gesture event JSON object}, label={lst:gesture}]
{"gesture": <gestureName>, 
 "eventType": <"StartPreStroke" | "StartNucleus" | "StartPostStroke">,
 "phase": <"PreStroke" | "Nucleus" | "PostStroke">,
 "rightX": <rightHandXWorldCoordinate>,
 "rightY": <rightHandYWorldCoordinate> } 
\end{lstlisting}

\begin{lstlisting}[caption={Speech event JSON object}, label={lst:speech}]
{"speech": <speechTag>}
\end{lstlisting}

\section{Gesture Controlled Presentation}
To demonstrate the use of multimodal input recognition system in a real life
application, I developed a gesture controlled presentation
application\footnote{See
\url{http://groups.csail.mit.edu/mug/projects/gesture_kinect/} for demo
videos.}, acting as a client. The application is HTML-based and uses
the \texttt{reveal.js} framework\footnote{\url{http://lab.hakim.se/reveal-js/}}.
The framework's API allows me to control the
presentation directly (e.g., changing slides, showing an overview, pausing
the slide show) instead of controlling it by binding to mouse
events (as is commonly done in many previous demonstrations of gesture
control). There are only 2 or 3 buttons on
a mouse, and its functionality is confined to clicking and dragging. By
mimicking mouse events, one limits the full potential of gesture input
interface, because our hands have five fingers and are much more versatile than doing just clicking and
dragging. 

\subsection{Handling Different Categories of Gestures}
This application demonstrates the use of path and pose gestures and shows how
the system respond differently for discrete \textit{flow} and continuous
\textit{flow} gestures.
I map the gestures in the YANG dataset to presentation control actions
(Table~\ref{tab:map-gesture}). 

\begin{table}[tbh]
\centering
\begin{tabular}{|l|l|}
\hline
\thead{Gesture} & \thead{Action} \\
\hline
Swipe Left & Next slide (DF) \\
\hline
Swipe Right & Previous slide (DF) \\
\hline
Circle & Toggle overview (show all slides) (DF) \\
\hline
Horizontal Wave & Toggle pause (turn screen black) (DF) \\
\hline
Point & Show a round cursor corresponding to hand position (CF) \\
\hline
Palm Up & Show a square cursor and seek video forward or backward (CF) \\
\hline
\end{tabular}
\caption{Mapping from gestures to presentation control actions. DF stands for
discrete \textit{flow} gesture and CF stands for continuous \textit{flow}
gesture.}
\label{tab:map-gesture}
\end{table}

An interface
should reflect the system's current state of understanding of the users'
actions to help users develop the right mental model for interaction. For
discrete \textit{flow} gestures, the interface needs to respond at the
StartPostStroke event. This responsiveness of the system helps users understand the relation
between their actions and the system's responses. 

For continuous \textit{flow} gestures, the
interface needs to show visual feedback of frame by frame changes
corresponding to certain hand parameters. In addition, as pose gestures
have distinct poses but arbitrary movement, it is important to have different 
visualizations show the system's understanding of the different
poses. Hence, for a Point gesture, the system shows a round cursor moving
according to the user's hand position (Figure~\ref{fig:point-overview}), and for
a Palm Up gesture, it shows a square cursor (Figure~\ref{fig:palm-up}).
Listing~\ref{lst:client-code} shows the corresponding client code in
CoffeeScript.

\begin{figure}[tbh]
\centering
\includegraphics[trim=0 2.7cm 0 0,
clip, width=\columnwidth]{figures/video_control.PNG}
\caption{Square cursor for Palm Up gesture to seek video forward or backward by
moving hand left or right.}
\label{fig:palm-up}
\end{figure}

\begin{lstlisting}[caption={Client code mapping gesture events to actions in
CoffeeScript.}, label={lst:client-code}] 
switch ge.eventType
  when 'StartPostStroke'
    switch ge.gesture
      when 'Swipe_Left'
        if @_config.mirror then Reveal.right() else Reveal.left()
      when 'Swipe_Right'
        if @_config.mirror then Reveal.left() else Reveal.right()
      when 'Circle' then Reveal.toggleOverview()
      when 'Horizontal_Wave' then Reveal.togglePause()
  else
    switch ge.gesture
      when 'Point' then @_view.updateCirclePointer(ge.rightX, ge.rightY,
                                                   @_config.mirror)
      when 'Palm_Up' then @_view.updateSquarePointer(ge.rightX, ge.rightY,
                                                     @_config.mirror)
      when 'Rest' then @_view.reset()
\end{lstlisting}

\subsection{Gesture and Speech}
Traditionally, gestures are used to augment speech in interaction, as pioneered
by Bolt's ``Put That There'' system \cite{Bolt80}. My previous user study
\cite{yin10thesis} shows that speech also can augment gestures.

When using gesture to augment speech, gesture is often used to indicate
location. This type of gesture, also called deictic gesture (a pointing
gesture), is very effective in conveying spatial information, and is often used to
supplement a spoken deictic marker (this, those, here, etc.)~\cite{oviatt97,
tse07}.
In my application, the user can show a slide by pointing at it in the overview and
saying ``show this slide'' (Figure~\ref{fig:point-overview}). Synonyms of the
verb (e.g., ``display'', ``go to'') can be used as well to improve flexibility
and naturalness.

\begin{figure}[tbh]
\centering
\includegraphics[width=\columnwidth]{figures/point_overview.PNG}
\caption{In the overview mode, user can point to a slide and say ``show this
slide'' to display it.}
\label{fig:point-overview}
\end{figure}

When speech is used to augment gesture, the spoken words are adjectives and
adverbs to modify actions indicated by gestures~\cite{yin10thesis}. This
combination is particularly useful when there is a limitation in the
expressiveness of the gesture or in the physical space of the gesture. In my
application, when using the Palm Up gesture to move a video forward and
backward the user can say ``faster'' or ``slower'' or their synonyms to
control video speed.

The speech recognition in my system is based on key word spotting by defining
grammars in XML (Listing~\ref{lst:grammar}). The application needs to
connect a behavior to each speech tags.

\begin{lstlisting}[caption={An example of grammar definition in XML.},
language=XML, label={lst:grammar}] 
<item>
  <tag>SHOW</tag>
  <one-of>
    <item>show</item>
    <item>open</item>
    <item>display</item>
    <item>go to</item>
  </one-of>
</item>
\end{lstlisting}

When speech and gesture are used together, they not only complement each
other, but also reinforce each other. In the 
presentation application, the behavior to speech events is dependent on gesture
event (see Listing~\ref{lst:speech-code}). The dependency helps to reduce false
positives such that if the user says some command words defined in the grammar 
without doing the gesture, the system will not respond. This allows the user to
say those words freely in other context.

\begin{lstlisting}[caption={Code for speech events in CoffeeScript.},
label={lst:speech-code}] 
switch speechText
  when 'MORE'
    @_view.onMore() if @_currentGesture is 'Palm_Up'
  when 'LESS'
    @_view.onLess() if @_currentGesture is 'Palm_Up'
  when 'SHOW'
    @_view.onShowSlide() if @_currentGesture is 'Point'
\end{lstlisting}

In many commercial speech input interface, a trigger action is required to tell
the system that the following speech is intended as a command. The trigger
action can either be actions such as holding down a button (e.g., iPhone's
Siri interface and Google Now) or specific speech utterance (e.g., ``OK Glass''
for Google Glass and ``Xbox'' for Xbox Kinect voice commands). Using a
combination of speech and gesture provides an alternative and more natural way
to engage the system for voice command.

\subsection{Natural Direction}
The natural direction of interaction depends on the relative position of the
display, the user and the sensor. There are two common modes of interaction:
users facing the display (e.g., controlling TV); or users facing away from the
display (e.g., doing presentation). If the user is facing the display, the UI should show mirrored
effects.
For example, a Swipe Left gesture would naturally corresponds to ``go to the next slide'' action. On the other hand, if
the user and the display face the same direction
(Figure~\ref{fig:presentation}), the Swipe Left gesture would corresponds more
naturally to ``go to the previous slide'' action.

\begin{figure}[tbh]
\centering
\includegraphics[width=0.5\textwidth]{figures/circle1.png}
\caption{The user and the display face the same direction during presentation.}
\label{fig:presentation}
\end{figure}

As a result, the UI responses
need to adapt to the sensor and the display coordinate spaces
(Figure~\ref{fig:coordinate}).
For example, if the display and the sensor are facing in the same direction,
when the user points towards the positive $x$ direction in the sensor's coordinate space, the corresponding cursor on the display should also
move to the positive $x$ direction in the display's coordinate space 
(Figure~\ref{fig:display-behind-sensor}).
On the other hand, if the display and the sensor are facing the opposite
directions, when the user points towards the positive $x$ direction in the
sensor's coordinate system, the corresponding cursor should move to the negative
$x$ direction in the display's coordinate space
(Figure~\ref{fig:display-in-front-sensor}). My system can be easily configured
using a flag for these two modes (see
Listing~\ref{lst:client-code}).

\begin{figure}[tbh]
\centering
\subfigure[Display and sensor face the same direction.]{
\includegraphics[width=0.45\columnwidth]{figures/coordinate_behind.png}
\label{fig:display-behind-sensor}
}
\subfigure[Display and sensor face the opposite directions.]{
\includegraphics[width=0.5\columnwidth]{figures/coordinate_front.png}
\label{fig:display-in-front-sensor}}
\caption{Sensor and display coordinate spaces.}\label{fig:coordinate}
\end{figure}

\section{Adding New Gestures}
Listing~\ref{lst:gesture-def} shows an example of the gesture definition file
the system uses. To add a new gesture, the user just needs to add a new line to
the file specifying the name of the gesture, the form of the gesture (D for
dynamic path gesture, S for static pose gesture), whether the gesture has
arbitrary number of repetitions like the Wave gesture (0 means no repetition)
and the number of hidden states per gesture (for pose gestures, the number of hidden states must be 1).
Then the user can specify the number of examples per gesture to provide and
start the training process (Figure~\ref{fig:training}).
The system records the new gesture examples and updates the gesture model to
include the new one.

\begin{lstlisting}[caption={Gesture definition file.},
label={lst:gesture-def}] 
#name,            form, repeat, nHiddenStates
Swipe_Left,         D,    0,      4
Swipe_Right,        D,    0,      3
Circle,             D,    0,      4
Horizontal_Wave,    D,    1,      4
Point,              S,    0,      1
Palm_Up,            S,    0,      1
Next_Point,         D,    0,      3
\end{lstlisting}

\begin{figure}[tbh]
\centering
\includegraphics[width=0.5\columnwidth]{figures/training_interface.PNG}
\caption{Training interface.}
\label{fig:training}
\end{figure}

\section{Discussion}
The client server architecture provides a clean separation of the user
interface and the back-end recognition engine, hiding the complexity of the
multimodal input recognition from application developers. The gesture event
based API gives a rich set of input compared to the mouse events (e.g., mouse
entered, mouse clicked, mouse dragged), and allows easy mapping to application
functionality. The limited functionality of a mouse forces the
Windows-Icons-Menus-Pointer (WIMP) paradigm in UI design that has not changed
for decades. With a new richer set of gesture input, we open up the
potential for designing simpler,  more natural, and more intuitive user
interfaces.


%user adaptation
