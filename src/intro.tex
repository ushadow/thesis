\begin{savequote}
Gesture is a critical link running through the evolution of perception,
conceptualization, and language.
\qauthor{David Armstrong, William Stokoe, and Sherman Wilcox, \textit{Gesture
and the nature of language}}
\end{savequote}
\chapter{Introduction}
% (Start with a motivating scenario: either Powerpoint presentation or USAR
% application.)
% \begin{quotation}
% \textit{Imagine an earthquake has hit a populous area and many major roads are
% impassable. Buildings are severely damaged or collapsed, and fire has broken out
% in many places. At the crisis management center, response professionals are
% working together to coordinate the earthquake relief effort. You are the
% incident commander charged with coordinating the search
% and rescue teams working in the field. There is a large tabletop display in
% front of you, showing the map of the site. Information coming from the field is
% updated on the display in real-time.}
% 
% \textit{A report about a big explosion at a chemical plant comes in and you move
% the map around, zoom in and rotate it to get a good view of the plant. On the
% map, you see there is a group of unmanned vehicles nearby. After selecting them
% on the map with your hand, you speak to the interface ``Go nearer to the
% explosion site to gather more information,'' while tracing the route the
% vehicles should take to avoid obstacles. Then you instruct rescue team No. 3 to evacuate the residents
% in the surrounding buildings by going under a bridge because the surface of the
% bridge is blocked. You gesture with one hand as the bridge and the other
% hand moving under it to emphasize this.}
% \end{quotation}
% 
% The scenario above is an example in the Urban Search and Rescue (USAR) domain.
% It shows an application of a multi-modal interface to a real-world problem. Gestures play an important part in this scenario,
% providing key information about location, method and timing of movements,
% and about spatial relationship among the objects being described.
\begin{quotation}
Imagine how nice it would be, the next time you make a presentation, if you did not need to stand close to your laptop or use a remote control with its limited functionality. What if you could present your work as naturally as having a conversation with your audience. You swipe your hand left and right to change slides. When you point to the slide with your hand, the display shows a pointer cursor following wherever you point. When you are showing a video, you use a palm forward hand pose (�stop� gesture) to pause the movie, then move left and right to fast forward or rewind the video. You can also say �faster� or �slower� to change the video speed. When you need to jump to a particular slide, you make a circle gesture to show all the slides, and say "show this slide'' while pointing at that slide. You can also make a dismiss gesture to pause the slide show (making the screen black) to take the distracting slides off the screen and get full attention from the audience during presentation.

\end{quotation}

The above scenario shows an application of a multimodal interface to a
real-world problem, with different types of gestures playing an important part
in the scenario.
 
Recent trends in user interfaces have brought on a new wave of interaction
techniques that depart from the traditional mouse and keyboard that have been 
used for decades. These include multi-touch interfaces such as the 
iPhone, the iPad and the Microsoft 
Surface\textsuperscript{\textregistered} as well as camera-based systems such as
the Microsoft Kinect and the Leap Motion sensor. Most
of these devices gained instant popularity among consumers, and the common trait
among them is that they make interacting with computation more natural and 
effortless. All these devices allow users to use their hands and/or body 
gestures to directly manipulate virtual objects. It feels more natural this 
way because this is how we interact with our environment in everyday life.
 
There is also a trend in wearable human-computer interfaces recently pioneered
by products such as Google Glass, Samsung's Galaxy Gear smartwatches and Pebble
smartwatches. These products have potential for gesture input as well. Google
Glass has a camera which can be used to recognize hand motion and hand shapes.
The accelerometers in the smartwatches can also be used to measure hand motion,
just like the Nintendo Wii controller.

There are big potential and demand for natural interaction, and gesture is
an important part of it. We start to see more and more gestural interfaces,
however, many of them still mainly make the hands function as a mouse with a
limited number of other gestures. Our goal is to break out from the old
paradigm of ``point, click, drag'' interaction we used to have with mice. 
Our hands are much more versatile, and hence, I believe that to design a gesture
recognition system for natural human computer interaction (HCI), we need to
start from the user interaction perspective: what are the different types of
gestures people use; when should the system respond; how should the model be
defined and trained; and how to combine gesture and speech for natural
interaction. These are the questions I addressed in this thesis. Based on my
findings, I developed a real-time continuous gesture recognition and
interaction system that handles different types of gestures seamlessly, and
responds to gestures appropriately.

\section{Background}
To design a natural gestural interface, it is important to understand the
nature of human gestures. This section gives some background information in
gesture production and taxonomy, and introduces several important concepts and
terms that are central to the final system design.
 
\subsection{Definition of Gestures}
Webster's Dictionary defines gestures as ``\ldots a movement usually of the body or limbs
that expresses or emphasizes an idea, sentiment, or attitude.'' This definition
is particularly related to the communicational aspect of the human hand and body
movements. However, in HCI, the notion
of gestures is somewhat different. In their review of the visual interpretation
of hand gestures for HCI, Pavlovic et al. \cite{Pavlovic97} states that in a computer
controlled environment one wants to use the human hand to perform tasks that
mimic both the natural use of the hand as a manipulator, and its use in
human-machine communication. They include both manipulative and communicative
gestures in their gesture taxonomy. This is also the definition I use in
the thesis.

Pavlovic et al. \cite{Pavlovic97} also gives a model (Figure. 
\ref{fig:gesture_production}) for the production and perception of gestures 
based on the model used in the field of spoken language recognition. According 
to their model, gestures originate as a gesturer's mental concept, possibly in 
conjunction with speech. They are expressed through the motion of arms and 
hands. Also, observers perceive gestures as streams of visual images which they
interpret using their knowledge about those gestures. In HCI, the 
observer is the computer and the knowledge it possesses is the training data we 
give it.

\begin{figure}[tbh]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/gesture_production.png} 
  \caption{Production and perception of gestures. Hand gestures originate as a
  mental concept, are expressed through arm and hand motion, and are perceived
  as visual images \cite{Pavlovic97}.}
  \label{fig:gesture_production}
\end{figure}

Kendon~\cite{kendon86} distinguishes \textit{autonomous gestures} from
\textit{gesticulation}, gestures that occur in association with speech. We focus
on autonomous gestures.

\subsection{Gestural Taxonomy}\label{sec:taxonomy}
The mental concept of a gesture represents the semantic meaning or the
actual intention of the gesturer. In this dimension, hand movements can be
divided into different general categories. Each category has its own characteristics and requires different responses from the computer interface. Having a systematic gesture taxonomy can inform
us about the design of the interactive system. The taxonomy will also be the
basis of our hierarchical approach to gesture interpretation. Our gesture
taxonomy (see Figure \ref{fig:taxonomy}) is based on the one developed by Pavlovic et al.
\cite{Pavlovic97} with some changes to make the terminology clearer. This is
also similar to the \textit{nature} dimension in the taxonomy proposed by
Wobbrock et al. \cite{wobbrock09}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/taxonomy.png} 
  \caption{Gesture taxonomy}
  \label{fig:taxonomy}
\end{figure}

First, gestures, as \textit{intentional} movements, should be distinguished from
\textit{unintentional} hand movements (like beats). By \textit{unintentional}
movements, we mean movements that are not intended to convey information. In
contrast, gestures are meaningful hand movements that people make to convey some
information. The distinction is important for a natural interface because there
should not be any restriction on how people should place or move their hands
when they are not doing any meaningful gestures. 

Gestures are then further divided into manipulative and communicative
categories. Manipulative gestures are used to act on objects (e.g.
moving an virtual object around, click a button, the property of the virtual
object change directly according to certain parameters of the hand(s)), while
communicative gestures have an inherent purpose for communication \cite{Pavlovic97}. 
the meaning of the gesture when the user finishes performing the gesture. This part is
accomplished by gesture recognition. So manipulative gestures and communicative
gestures fall into the \textit{continuous} category and the
\textit{discrete} category respectively in the \textit{flow} dimension of the
taxonomy proposed by Wobbrock \cite{wobbrock09} et al. based on their
user study.

People perform communicative gestures via acts or symbols. Gesture via acts are
those directly related to the interpretation of the movement itself. Such
movements are classified as either mimetic (which imitate some actions) or
deictic (pointing acts that convey spatial information). Gestures via symbols
are those that have a linguistic role, and are often represented by different static hand postures. An example is forming the
O.K. pose for ``accept''. 

\subsection{Gesture Form and Flow}
Wobbrock et al. \cite{wobbrock09} classified gestures into four dimensions:
\textit{form}, \textit{nature}, \textit{binding}, and \textit{flow}. These
dimensions are orthogonal. We looked into the \textit{form} and the
\textit{flow} dimensions. The \textit{form} dimension is particularly important
for building the gesture recognition model because different forms constitute
different features we need to consider. The \textit{binding} and \textit{flow}
dimensions are relevant for the application layer because they are related to
how the user interface should respond to gesture events.

However, we take a different definition on gesture with dynamic paths. In their
paper, they refer dynamic paths as any gestures that involve change of hand
locations. However, we consider gestures with dynamic paths as gestures with a
well-defined paths. For example, a ``Swipe Left'' gesture has a dynamic path,
where as a ``Pointing'' gesture with moving hand to indicate different locations
is not considered as gestures with dynamic path because the path is not
well-defined (the user is likely to move anywhere as he/she wants). The reason
for this distinction is that gestures with well-defined paths are suitable to be
model by sequential data models such as HMMs.

The physical arm and hand movements that express
the mental concept of the gesture can be categorized into different \textit{forms} \cite{wobbrock09} as shown in Table \ref{tab:form}. Both manipulative and communicative gestures can
have all of these forms. This means that the features we want to use for
gestural analysis should be based on both the hand pose and the locus of the
hand movement.

\begin{table}[h]
  \centering
  \begin{tabular}{| l | l |}
  	\hline
  	\textbf{Form} 		  & \textbf{Description} \\ \hline 
  	Static pose  		  & Hand pose is held in one location. \\ \hline
  	Dynamic pose 		  & Hand pose changes while location does not. \\ \hline
  	Static pose and path  & Hand pose is held as hand moves. \\ \hline
  	Dynamic pose and path & Hand pose changes as hand moves. \\ \hline
  \end{tabular}
  \caption{Different gesture forms.}
  \label{tab:form}
\end{table}

To respond properly, the system must differentiate discrete and continuous
flow gestures. If a gesture's flow
is discrete, the whole sequence needs to be delimited, recognized, and responded
as a single event. For example, a \textit{wave} gesture that can mean ``no'' or getting the attention of the 
system should be responded at the end of the gesture. On the other hand, if the 
flow is continuous, ongoing system response is required. Consider the \textit{pan} and the \textit{resize} gestures, 
as the user moves his/her hand(s), the system has to respond in each frame such that
certain parameter(s) of the virtual object(s) that the user is controlling are tied to
certain parameter(s) of the user's hand(s).

One major distinction between discrete and continuous flow gestures is that,
in a pre-defined gesture set, discrete flow gestures usually have specific hand
poses and movement paths, where continuous flow gestures usually do not have
specific movement paths, but can have specific hand poses.
For example, when panning, the user's hand may move in any
direction. As a result, 
 we cannot train the system with examples of  the
\textit{pan} gesture in only one direction.

\subsection{Temporal Modeling of Gestures}
Since human gestures are a dynamic process, it is important to consider the
temporal characteristics of gestures. This may help in the temporal segmentation
of gestures from other unintentional hand/arm movements \cite{Pavlovic97}. It
has been established that three phases make a gesture:
\begin{itemize}
  \item preparation,
  \item nucleus (peak or stoke \cite{mcneill82}), and
  \item retraction \cite{Pavlovic97}.
\end{itemize}
The three temporal phases are distinguishable through the general hand/arm
motion: ``Preparation'' and ``retraction'' are characterized by the rapid change
in the position of the hand, while the ``nucleus'' or the ``stroke'', in
general, exhibits relatively slower hand motion. The ``stroke'' of a gesture, as
Kendon \cite{kendon86} observes, has some ``definite form and enhanced dynamic
qualities''. Every gesture must have a ``stroke'', which is considered by Kendon
to be the content-carrying part of the gesture. Based on this theory, we
expect that he lack of the ``stroke'' part is how unintentional movements can
be distinguished from gestures. However, Kendon does not explain exactly what
the dynamic qualities are. Hence, this is what this thesis is going to explore, i.e., what forms and dynamic qualities differentiate gestures from unintentional movements.

\section{Thesis Outline and Contributions}
The gesture interaction system consists of the following modules: hand tracking, 
feature extraction, gesture recognition, and application user interface
(Figure~\ref{fig:overview}). This thesis describes new methods and
contributions in each module to improve the gesture recognition accuracy and
user experience. The key elements are:

\begin{figure}[tbh]
\centering
\includegraphics[width=0.7\linewidth]{figures/system_overview.png}
\caption{System overview.}
\label{fig:overview}
\end{figure}

\begin{itemize}
  \item \textbf{Improved hand tracking methods based on gesture salience.}
  I define gesture salience to be proportional to the amount of motion and the
  closeness of the motion to the observer. Based on this, I compute a
  probability map for the gesturing hand locations in a frame. This method
  gives better result than the current Kinect SDK when the hands are closer to
  the body or are moving fast.

  \item \textbf{Fine-grained hand feature extraction.} To handle manipulative
  gestures, I developed algorithms to estimate a simplified 3-D
  skeletal hand model with fingertip positions. For communicative gestures, I am
  comparing different feature descriptors (e.g., histogram of oriented
  gradients) and feature encoding schemes (e.g., principal component analysis,
  sparse coding) for incorporating hand poses into the recognition framework.

  \item \textbf{A unified probabilistic model for gesture recognition.} I am
  developing a framework based on hierarchical hidden Markov
  models (HHMMs) that distinguishes non-gestures, 
  manipulative gestures and communicative gestures in realtime. I am also experimenting with different
  training methods, such as discriminative training and hidden conditional
  random fields, to improve the recognition accuracy. 

  \item \textbf{Online gesture spotting.} A real-time gesture-driven HCI
  application requires gesture spotting with minimum delay. I developed a new
  method for determining the start and the end of a gesture nucleus in
  real-time, while filtering out unintentional hand movement based on
  information from hidden states. Identify the use of hidden states which are
  not done before. Before people just look at the likelihood of the whole
  sequence.
  
  \item \textbf{User interaction techniques.} I identified the importance of
  having both discrete and continuous gestures in a natural interface and how to
  combine them in an interaction interface. I also explored different ways of
  combining gesture and speech for natural interaction.
\end{itemize}

\section{Gesture Taxonomy for Natural Interaction}\label{sec:taxonomy}
Several gesture taxonomies have been suggested in the literature. Some of them
deal with psychological aspects of gestures \cite{kendon86, mcneill82}, while
others are inspired by an HCI perspective \cite{quek94,
quek95, Pavlovic97}. The taxonomy that seems most appropriate for natural HCI
and human-centric design was developed Wobbrock et al. \cite{wobbrock09}. Their
study is based on eliciting natural behavior from non-technical users when
interacting with a computing system. Although their user study focuses on
tabletop gestures, we believe that the results can be generalized to other
interactive interfaces. 

Wobbrock et al. classify gestures in four orthogonal dimensions. As a first
step, we focus on two of them (Table~\ref{tab:taxonomy}) and incorporate them in
designing the gesture recognition system and interface. We also further generalize the
taxonomy to encompass interaction for both vertical and horizontal displays.

\subsection{Gesture Forms and Flows}
One dimension is the \textit{form} of a gesture; we distinguish two
categories in this dimension: path and pose. The first category -- path --
contains gestures characterized by distinct paths without any distinct
hand pose. For example, a ``swipe left'' gesture is characterized by a
right to left motion, while a ``circle'' gesture is characterized by a
circular motion of the hand. In doing these, users typically hold their
hands in some natural relaxed pose. The second category of gestures is
characterized by distinct hand poses without any distinct paths. This category
of gestures is usually associated with direct manipulation of some virtual
objects on the interface.
For example, a user may use a ``point'' hand pose and move around to
point at different things on a display.

\begin{table}[tbh]
\centering
\begin{tabular}{|c|l|l|}
\hline
\multirow{2}{*}{\textbf{\textit{Form}}} & \textit{distinct path} & with any hand
pose
\\
\cline{2-3} 
                               & \textit{distinct hand pose} & with any path \\
\hline
\multirow{2}{*}{\textbf{\textit{Flow}}} & \textit{discrete} & response occurs
\textit{after} the user acts \\
\cline{2-3}
              & \textit{continuous} & response occurs \textit{while} the user
              acts \\
\hline
\end{tabular}
\caption{Taxonomy of gestures for natural interaction.}
\label{tab:taxonomy}
\end{table}

Another dimension is the \textit{flow} of a gesture. A gesture's flow is
discrete if the gesture is performed, delimited, recognized, and responded to
as an \textit{event} \cite{wobbrock09}. One example is the ``wave'' gesture
which has a few repetitions of left and right movement; usually we want the system to
respond at the last repetition. Flow is continuous if ongoing recognition is required
and the system should respond frame by frame, as for example during a
``point'' gesture, where we want to show the cursor on the screen
continuously moving according to the hand position. 

\subsection{Temporal Modeling of Gestures}
Making gesture interaction feel natural requires a system that responds at the
correct moment. As a result, it is important to consider the temporal
characteristics of gestures. We set a foundation for doing this by taking
account of the three phases that make of a gesture:
\begin{itemize}
  \item pre-stroke,
  \item nucleus (peak \cite{mcneill82}), and
  \item post-stroke \cite{Pavlovic97}.
\end{itemize}

``Pre-strokes'' and ``post-strokes'' are movement from and to the
rest position. The ``nucleus'' of a gesture,
as Kendon \cite{kendon86} observes, has some ``definite form and enhanced dynamic
qualities''. Every gesture must have a nucleus, which is the content-carrying
part of the gesture. Based on this theory, we expect that the lack of the
nucleus phase is how unintentional movements can be distinguished from gestures. 

Even though the end of the
post-stroke phase can be more easily detected by finding the start of the
rest position, we want to do more than this to. Since nucleus is the meaningful
part of the gesture, for a discrete flow gesture, we want the system to respond immediately at the end of the nucleus
phase instead of at the end of the post-stroke phase. To make the system more responsive,
we address the more challenging problem of detecting the start and end of the nucleus phase from the pre-stroke
and post-stroke phases. This also allows the system to respond to continuous
flow gesture immediately at the start of the nucleus phase.

\section{Contributions}
The main contributions of this work include:
\begin{itemize}
  \item We developed a unified probabilistic framework for real-time gesture
  recognition that combines two forms of gestures.
  \item We use embedded training and hidden state information to detect
  different gesture phases, allowing the system to respond more promptly.
  \item We collected a new dataset that include two forms of gestures, a
  combination currently lacking in the community. We also propose a hybrid
  evaluation metric that is more relevant to real-time interaction and different
  types of gestures.
\end{itemize}

\section{System Overview}
We develop our system based on the understanding of the gestures for
natural interaction. 
We use a RGB-depth sensor (the Kinect sensor) for hand tracking so that we can extract
features for hand poses.

The gesture recognition model estimates the current most likely gesture label
and gesture phase information based on the input stream of
feature vectors. The gesture information, 
together with smoothed hand position information are sent to the application level at each time frame.

\section{Notations}
or in appendix

% \subsection{Discriminative Training}
% HMMs as a generative model allows us to model the joint
% distribution of the observed sequence. Traditionally,
% maximum-likelihood (ML) estimation is used to learn the parameters of HMMs. However, our real task is to
% classify the sequence of the observed data. Discriminative classifiers model the
% posterior $p(y|x)$ directly, and hence are believed to give a lower error rate.
% In the speech recognition community, discriminative training methods have been
% proposed and have shown significant improvements over ML-trained models on many
% large vocabulary speech recognition tasks \cite {chang12}. To further improve
% the discriminative power of our model, we will use discriminative training
% methods for gestural analysis.
% 
% HMM provides a computationally efficient modeling framework. The HMM makes two
% main assumptions:
% 
% 1. There exists a hidden state sequence, $S$, that forms a Markov chain that
% generates the observation vectors.
% 
% 2. Given the state $s_t$ at any time point, the corresponding observation $x_t$
% is conditionally independent of other observations and states.
% 
% Instead of seeking model parameters that can maximize the likelihood of the
% data, discriminative training methods seek parameters that can minimize the
% confusions that occur in the training data. In general, discriminative training
% methods consist of two steps: First, construct a smooth and efficient computable
% objective function that reflects the degree of confusion; second, adjust the
% model parameters such that the objective function can be optimized. 
% 
% We will explore several commonly used discriminative training criteria such as
% minimum classification error training, maximum mutual
% information training, and margin-based training methods. For optimizing
% parameters, we can use either the extended Baum-Welch method or gradient-based
% methods. We will also compare the results to those without discriminative
% training and those with CRF-based models.

% \section{Appendix A - Review of Kalman Filter}
% Let $x_k$ be an $n$-dimensional vector of state components and $P_k$ be the
% $n$-by-$n$ error covariance. The measurement $z_k$ is an $m$-dimensional
% vector given by:
% \begin{align*}
% z_k = H_kx_k + v_k,
% \end{align*}
% where $H_k$ is an $m$-by-$n$ matrix and $v_k$ is the measurement error.
% 
% The \textit{Kalman gain}, $K_k$, is an $n$-by-$m$ matrix expressed as:
% \begin{align*}
% K_k = P_k^-H_k^T(H_kP_k^-H_k^T + R_k)^{-1}
% \end{align*}
% 
% Assuming no external control, the a priori estimate $x_k^-$ of the state is
% given by:
% \begin{align*}
% x_k^- = Fx_{k - 1} + w_k,
% \end{align*}
% where $F$ is the $n$-by-$n$ \textit{transfer matrix} characterizing the
% dynamics of the system, and $w_k$ is the \textit{process noise} associated with
% random events or forces that directly affect the actual state of the system. We assume that the components of $w_k$
% have Gaussian distribution $N(0, Q_k)$ for some $n$-by-$n$ covariance matrix
% $Q_k$.
% 
% Using $P_k^-$ to denote the error covariance, the a priori estimate for this
% covariance at time $k$ is obtained from the value at time $k - 1$ by:
% \begin{align*}
% P_k^- = FP_{k - 1}F^T + Q_{k - 1}
% \end{align*}
% 
% The updated value for $x_k$ when a new measurement is available is:
% \begin{align*}
% x_k = x_k^- + K_k(z_k^- - H_kx_k^-)
% \end{align*}
% The update value for $P_k$ is:
% \begin{align*}
% P_k = (I - K_kH_k)P_k^-
% \end{align*}
% 

