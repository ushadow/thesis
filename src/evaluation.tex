\chapter{Online Recognition Evaluation}\label{sec:evaluation}


\section{Real-time System Evaluation}
I evaluate the performance of my real-time gesture recognition using the YANG
dataset, and the hybrid performance
metrics proposed in Section~\ref{sec:metrics}. The evaluation is based on the
assumption that all the path gestures (gesture \#1--4) are
discrete flow gestures, and pose gestures (gesture \#5--7) are continuous flow
gestures.
The survey results in Section~\ref{sec:preferences} show that it is
important to allow users to quickly define and train their own gestures. Hence,
I evaluate my system using user dependent training and testing. For each user
in the dataset, I use the first 2 sessions of recording (6 samples per gesture)
as training examples, and the last 2 sessions as testing examples.
Figure~\ref{fig:recog-result} shows a visualization of the recognition result on
a test sequence. The figure highlights the challenges in the test sequences:
there are 21 gestures in each continuous unsegmented sequence; sometimes
gestures immediately follow one another.

The average test results for 10 users are reported.

\begin{figure}[thb]
\centering
\includegraphics[trim=10mm 5mm 10mm 5mm, clip,
width=\columnwidth]{figures/recog_result_m3.eps}
\caption{Comparison between recognition result using online inference
and ground truth.
The colors correspond to different gestures. For discrete flow gestures
(swipe left/right, circle, horizontal wave), one color segment with a fixed
length is shown at the time of response. For continuous flow gestures, the
recognized gesture is shown at each frame indicating frame-by-frame responses.}
\label{fig:recog-result}
\end{figure}

\section{Compare Different Numbers of Principal Components}
With everything else the same, Figure~\ref{fig:pca} shows the recognition $F_1$
scores with changing number of principal components used for the HOG
descriptor.
The best average score is obtained when 15 principal components are used.

\begin{figure}[tbh]
\centering
\includegraphics[width=\columnwidth]{figures/f1_pca.png}
\caption{Graph showing how $F_1$ scores for discrete flow gestures, continuous
flow gestures and the average scores change with the number of principal
components used for the HOG descriptor.}
\label{fig:pca}
\end{figure}

\section{Compare Different Topologies}
In this section, I compare my unified framework with a common HMM-based approach
used in previous works~\cite{sharma00, Starner95}, i.e., using the same
left-right topology for all gestures.

Table~\ref{tab:result} compares the results between the two methods.
The third column is the result from treating the two forms of gestures in the
same way, i.e., all gestures have the same left-right Bakis model for their nucleus
phases. The forth column is the result from using a left-right Bakis model for
path gestures and a single state for pose gestures. To have a fair comparison,
all hidden states have 3 mixtures of Gaussians. As Table~\ref{tab:result}
shows, having different HMM topologies for the two forms of gestures significantly increases the recall and $F_1$ score
for pose gestures.

\begin{table}[tbh]
\centering
\begin{tabular}{|l|l|p{3cm}|p{3cm}|}
\hline
& & \textbf{Same topology for two forms of gestures} & \textbf{Different
topologies for two forms of gestures} \\
\hline
\multirow{4}{4cm}{Path \& discrete flow gestures} 
& Precision & \textbf{0.84 (0.16)} & 0.82 (0.16) \\
\cline{2-4}
& Recall & 0.87 (0.17) & 0.87 (0.18)\\
\cline{2-4}
& $F_1$ & \textbf{0.85 (0.16)} &  0.84 (0.16)\\
\cline{2-4}
& Responsiveness (s) & 0.6 (0.3) & 0.6 (0.3) \\
\hline
\multirow{4}{4.5cm}{Pose \& continuous flow gestures}
& Precision & \textbf{0.65 (0.14)} & 0.63 (0.11) \\
\cline{2-4}
& Recall & 0.41 (0.15) & \textbf{0.65 (0.09)} \\
\cline{2-4}
& $F_1$ & 0.50(0.14) & \textbf{0.64 (0.10)} \\
\hline
\textbf{Average} & $F_1$ & 0.675 (0.150) & \textbf{0.740 (0.130)} \\
\hline
\end{tabular}
\caption{Results from using different topologies. The numbers in parentheses are
standard deviations. The results are based on using 3 mixtures of Gaussians
for all hidden states, and lag time
$l = 8$ frames.}
\label{tab:result}
\end{table}

For gestures with arbitrary movement (e.g., pose gestures), it is difficult to
use embedded training to accurately align pre-stroke, nucleus and post-stroke phases.
Figure~\ref{fig:palm-hidden} illustrates this problem. It shows the most
likely hidden state estimates for a Palm Up gesture sequence. The main (center)
part of the gesture, which should be the nucleus of the gesture, is identified
as the post-stroke. This is why it is important to have different topologies and
different training strategies for the two forms of gestures.

\begin{figure}[tbh]
\centering
\includegraphics[width=0.3\linewidth]{figures/palm_hidden_label.png}
\caption{Estimated hidden states for a ``Palm Forward'' gesture using the
left-right model same as gestures with distinct path. Different colors correspond to
different hidden states.}
\label{fig:palm-hidden}
\end{figure}

\section{Compare Different Numbers of Mixtures}
The previous section shows that using different topologies for path and pose
gestures give better
recognition performance. So using this model, I investigate the effects
of the number of mixtures ($k$) in the MoG emission probabilities.

First, I set $k$ be the same for both forms of gestures.
Fig.~\ref{fig:mixtures} shows that the $F_1$ scores increases as the number of
mixtures increases until $k=3$.
After that, we start to see the effect of overfitting.

\begin{figure}[tbh]
\centering
\includegraphics[trim=10mm 5mm 10mm 15mm,
clip, width=\columnwidth]{figures/f1_nM.png}
\caption{F1 scores versus number of mixtures.}
\label{fig:mixtures}
\end{figure}

Then, I experimented with using different $k$'s for path and pose gestures. I
set $k^\text{path} = 3$ for path gestures, and use a different
$k^{\text{pose}}_g\in \{3\ldots6\}$ for pose gesture $g$. Each
$k^{\text{pose}}_g$ is chosen according to Section~\ref{sec:pose-gesture}.
Using this method, I am able to improve precision, recall and $F_1$ scores for
both forms of the gestures even further (see
Table~\ref{tab:different-mixtures}).

\begin{table}[t]
\centering
\begin{tabular}{|p{4.5cm}|l|p{4cm}|}
\hline
& & Use different topologies and numbers of mixtures \\
\hline
\multirow{4}{4cm}{Path \& discrete flow gestures} 
& Precision & \textbf{0.94 (0.05)} \\
\cline{2-3}
& Recall    & \textbf{0.93 (0.08)} \\
\cline{2-3}
& $F_1$ & \textbf{0.93 (0.06)} \\
\cline{2-3}
& Responsiveness (s) & 0.6 (0.2)  \\
\hline
\multirow{3}{4cm}{Pose \& continuous flow gestures}
& Precision & \textbf{0.68 (0.10)} \\
\cline{2-3}
& Recall & \textbf{0.69 (0.08)} \\
\cline{2-3}
& $F_1$ & \textbf{0.68 (0.09)}  \\
\hline
\textbf{Average} & $F_1$ & \textbf{0.805 (0.075)}\\
\hline
\end{tabular}
\caption{Results from using different numbers of mixtures of Gaussians
for the emission probabilities ($l = 8$ frames).}
\label{tab:different-mixtures}
\end{table}

%Compare user dependent vs user independent result

\section{Compare Different Lag Times}
Fig.\ref{fig:lag} shows how the $F_1$ scores change with respect to the lag
time ($l$) in fixed-lag smoothing. The performance increases as $l$ increases, and
plateaus at $l=8$ frames which is about 0.3s at 30 FPS. This shows that more
evidence does help to smooth the estimates until a limit, and we do not need to
sacrifice too much delay to reach the limit. Our result also shows that the
responsiveness score stays around 0.6--0.7 seconds as $l$ is increased.

\begin{figure}[tbh]
\centering
\includegraphics[trim=0 5mm 0 15mm, clip,
width=\columnwidth]{figures/f1_lag.png}
\caption{$F_1$ scores versus lag time $l$.}
\label{fig:lag}
\end{figure}

\section{Training Time}
The HMM-based unified framework is fast to train as well. The average
computation time for training the model for one user is about 5s with 7 gestures and 6 training examples per
gesture.

\section{Discussion}
Our overall best performance for the YANG dataset is reported in
Table~\ref{tab:different-mixtures}. The performance for the pose and
continuous flow gestures is relatively low compared to that of the path and
discrete flow gestures. There are big variations in the recognition results for
the pose gestures among users: the highest $F_1$ score for pose gestures is 0.81
and the lowest is 0.58. For the ones with low $F_1$ scores, most
confusions are among the 3 pose gestures, especially when there is significant
motion blur (see Figure~\ref{fig:point_grab}). These are users who move
relatively fast when doing the pose gestures. This suggests that users may need to perform pose gestures relatively slowly in order to get better recognition results.

\begin{figure}[tbh]
\centering
\includegraphics[width=\linewidth]{figures/point_posture_full_image.png}
\caption{This frame is mistakenly classified as Grab while the true gesture
is Point. Motion blur is significant.}
\label{fig:point_grab}
\end{figure}